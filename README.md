# ðŸ§© Awesome Token Reduction [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
A curated list of up-to-date papers on **token + {reduction, pruning, merging, compression, sparsification}** in transformers, large language models, large vision-language models, diffusion models, *etc*.


## 2025
- Exploring the Role of **Explicit Temporal Modeling** in Multimodal Large Language Models for Video Understanding <a href="https://arxiv.org/abs/2501.16786"><img src="https://img.shields.io/badge/arXiv-2501.16786-b31b1b?logo=arxiv" height="16"></a>
- **Contextual Reinforcement** in Multimodal Token Compression for Large Language Models <a href="https://arxiv.org/abs/2501.16658"><img src="https://img.shields.io/badge/arXiv-2501.16658-b31b1b?logo=arxiv" height="16"></a>
- **Dynamic Token Reduction** during Generation for Vision Language Models <a href="https://arxiv.org/abs/2501.14204"><img src="https://img.shields.io/badge/arXiv-2501.14204-b31b1b?logo=arxiv" height="16"></a>
- **LVPruning**: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models <a href="https://arxiv.org/abs/2501.13652"><img src="https://img.shields.io/badge/arXiv-2501.13652-b31b1b?logo=arxiv" height="16"></a>
- **InternVideo2.5**: Empowering Video MLLMs with Long and Rich Context Modeling <a href="https://arxiv.org/abs/2501.12386"><img src="https://img.shields.io/badge/arXiv-2501.12386-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5"><img src="https://img.shields.io/github/stars/OpenGVLab/InternVideo?label=InternVideo2.5&logo=github&style=flat-square" height="16"></a>
- **AdaFV**: Accelerating VLMs with Self-Adaptive Cross-Modality Attention Mixture <a href="https://arxiv.org/abs/2501.09532"><img src="https://img.shields.io/badge/arXiv-2501.09532-b31b1b?logo=arxiv" height="16"></a>
- **VASparse**: Towards Efficient Visual Hallucination Mitigation for Large Vision-Language Model via Visual-Aware Sparsification <a href="https://arxiv.org/abs/2501.06553"><img src="https://img.shields.io/badge/arXiv-2501.06553-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/mengchuang123/VASparse-github"><img src="https://img.shields.io/github/stars/mengchuang123/VASparse-github?label=VASparse&logo=github&style=flat-square" height="16"></a>
- **Compression with Global Guidance**: Towards Training-free High-Resolution MLLMs Acceleration <a href="https://arxiv.org/abs/2501.05179"><img src="https://img.shields.io/badge/arXiv-2501.05179-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/xuyang-liu16/GlobalCom2"><img src="https://img.shields.io/github/stars/xuyang-liu16/GlobalCom2?label=GlobalCom2&logo=github&style=flat-square" height="16"></a>
- **LLaVA-Octopus**: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding <a href="https://arxiv.org/abs/2501.05067"><img src="https://img.shields.io/badge/arXiv-2501.05067-b31b1b?logo=arxiv" height="16"></a>
- **TREAD**: Token Routing for Efficient Architecture-agnostic Diffusion Training <a href="https://arxiv.org/abs/2501.04765"><img src="https://img.shields.io/badge/arXiv-2501.04765-b31b1b?logo=arxiv" height="16"></a>
- **LLaVA-Mini**: Efficient Image and Video Large Multimodal Models with One Vision Token <a href="https://arxiv.org/abs/2501.03895"><img src="https://img.shields.io/badge/arXiv-2501.03895-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/ictnlp/LLaVA-Mini"><img src="https://img.shields.io/github/stars/ictnlp/LLaVA-Mini?label=LLaVA Mini&logo=github&style=flat-square" height="16"></a>
- **FOLDER**: Accelerating Multi-modal Large Language Models with Enhanced Performance <a href="https://arxiv.org/abs/2501.02430"><img src="https://img.shields.io/badge/arXiv-2501.02430-b31b1b?logo=arxiv" height="16"></a>
- What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the **Perspective of Graph** <a href="https://arxiv.org/abs/2501.02268"><img src="https://img.shields.io/badge/arXiv-2501.02268-b31b1b?logo=arxiv" height="16"></a>
- **FrameFusion**: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models <a href="https://arxiv.org/abs/2501.01986"><img src="https://img.shields.io/badge/arXiv-2501.01986-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/thu-nics/FrameFusion"><img src="https://img.shields.io/github/stars/thu-nics/FrameFusion?label=FrameFusion&logo=github&style=flat-square" height="16"></a>
- **Cached Adaptive Token Merging**: Dynamic Token Reduction and Redundant Computation Elimination in Diffusion Model <a href="https://arxiv.org/abs/2501.00946"><img src="https://img.shields.io/badge/arXiv-2501.00946-b31b1b?logo=arxiv" height="16"></a>
- **VideoChat-Flash**: Hierarchical Compression for Long-Context Video Modeling <a href="https://arxiv.org/abs/2501.00574"><img src="https://img.shields.io/badge/arXiv-2501.00574-b31b1b?logo=arxiv" height="16"></a>
- **Token Pruning for Caching Better**: 9Ã— Acceleration on Stable Diffusion for Free <a href="https://arxiv.org/abs/2501.00375"><img src="https://img.shields.io/badge/arXiv-2501.00375-b31b1b?logo=arxiv" height="16"></a>
- [ICASSP 2025] **Cross-Layer Cache Aggregation** for Token Reduction in Ultra-Fine-Grained Image Recognition <a href="https://arxiv.org/abs/2501.00243"><img src="https://img.shields.io/badge/arXiv-2501.00243-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/arkel23/CLCA"><img src="https://img.shields.io/github/stars/arkel23/CLCA?label=CLCA&logo=github&style=flat-square" height="16"></a>
- [AAAI 2025] Training-Free and Hardware-Friendly Acceleration for Diffusion Models via **Similarity-based Token Pruning** <a href="https://www.researchgate.net/profile/Linfeng-Zhang-18/publication/387204421_Training-Free_and_Hardware-Friendly_Acceleration_for_Diffusion_Models_via_Similarity-based_Token_Pruning/links/6763f5c78cfcdf077fe561e0/Training-Free-and-Hardware-Friendly-Acceleration-for-Diffusion-Models-via-Similarity-based-Token-Pruning.pdf"><img src="https://img.shields.io/badge/AAAI-2025-green" height="16"></a>

## 2024
- [TPAMI 2024] **Hierarchical Banzhaf Interaction** for General Video-Language Representation Learning <a href="https://arxiv.org/abs/2412.20964"><img src="https://img.shields.io/badge/arXiv-2412.20964-b31b1b?logo=arxiv" height="16"></a>
- **ReTaKe**: Reducing Temporal and Knowledge Redundancy for Long Video Understanding <a href="https://arxiv.org/abs/2412.20504"><img src="https://img.shields.io/badge/arXiv-2412.20504-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/SCZwangxiao/video-ReTaKe"><img src="https://img.shields.io/github/stars/SCZwangxiao/video-ReTaKe?label=ReTaKe&logo=github&style=flat-square" height="16"></a>
- [AAAI 2025] **ST3**: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming <a href="https://arxiv.org/abs/2412.20105"><img src="https://img.shields.io/badge/arXiv-2412.20105-b31b1b?logo=arxiv" height="16"></a>
- **Layer- and Timestep-Adaptive Differentiable Token Compression Ratios** for Efficient Diffusion Transformers <a href="https://arxiv.org/abs/2412.16822"><img src="https://img.shields.io/badge/arXiv-2412.16822-b31b1b?logo=arxiv" height="16"></a>
- [AAAI 2025] **ImagePiece**: Content-aware Re-tokenization for Efficient Image Recognition <a href="https://arxiv.org/abs/2412.16491"><img src="https://img.shields.io/badge/arXiv-2412.16491-b31b1b?logo=arxiv" height="16"></a>
- **PruneVid**: Visual Token Pruning for Efficient Video Large Language Models <a href="https://arxiv.org/abs/2412.16117"><img src="https://img.shields.io/badge/arXiv-2412.16117-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/Visual-AI/PruneVid"><img src="https://img.shields.io/github/stars/Visual-AI/PruneVid?label=PruneVid&logo=github&style=flat-square" height="16"></a> 
- Incorporating **Feature Pyramid Tokenization** and Open Vocabulary Semantic Segmentation <a href="https://arxiv.org/abs/2412.14145"><img src="https://img.shields.io/badge/arXiv-2412.14145-b31b1b?logo=arxiv" height="16"></a>
- **FastVLM**: Efficient Vision Encoding for Vision Language Models <a href="https://arxiv.org/abs/2412.13303"><img src="https://img.shields.io/badge/arXiv-2412.13303-b31b1b?logo=arxiv" height="16"></a>
- **Feather the Throttle**: Revisiting Visual Token Pruning for Vision-Language Model Acceleration <a href="https://arxiv.org/abs/2412.13180"><img src="https://img.shields.io/badge/arXiv-2412.13180-b31b1b?logo=arxiv" height="16"></a> <a href="https://web.stanford.edu/~markendo/projects/feather.html"><img src="https://img.shields.io/badge/Project-feather-yellow" height="16"></a> 
- Faster Vision Mamba is Rebuilt in Minutes via **Merged Token Re-training** <a href="https://arxiv.org/abs/2412.12496"><img src="https://img.shields.io/badge/arXiv-2412.12496-b31b1b?logo=arxiv" height="16"></a>
- **SpeechPrune**: Context-aware Token Pruning for Speech Information Retrieval <a href="https://arxiv.org/abs/2412.12009"><img src="https://img.shields.io/badge/arXiv-2412.12009-b31b1b?logo=arxiv" height="16"></a> <a href="https://speechprune.github.io/"><img src="https://img.shields.io/badge/Project-speechprune-yellow" height="16"></a>
- **AsymRnR**: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration <a href="https://arxiv.org/abs/2412.11706"><img src="https://img.shields.io/badge/arXiv-2412.11706-b31b1b?logo=arxiv" height="16"></a>
- **FTP**: A Fine-grained Token-wise Pruner for Large Language Models via Token Routing <a href="https://arxiv.org/abs/2412.11494"><img src="https://img.shields.io/badge/arXiv-2412.11494-b31b1b?logo=arxiv" height="16"></a>
- **OmniVLM**: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model for Efficient On-Device Inference <a href="https://arxiv.org/abs/2412.11475"><img src="https://img.shields.io/badge/arXiv-2412.11475-b31b1b?logo=arxiv" height="16"></a> <a href="https://huggingface.co/NexaAIDev/OmniVLM-968M"><img src="https://img.shields.io/badge/Huggingface-OmniVLM-yellow?logo=huggingface" height="16"></a> 
- [AAAI 2025] **Attention-driven GUI Grounding**: Leveraging Pretrained Multimodal Large Language Models without Fine-Tuning <a href="https://arxiv.org/abs/2412.10840"><img src="https://img.shields.io/badge/arXiv-2412.10840-b31b1b?logo=arxiv" height="16"></a>
- Memory Efficient Matting with **Adaptive Token Routing** <a href="https://arxiv.org/abs/2412.10702"><img src="https://img.shields.io/badge/arXiv-2412.10702-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/linyiheng123/MEMatte"><img src="https://img.shields.io/github/stars/linyiheng123/MEMatte?label=MEMatte&logo=github&style=flat-square" height="16"></a> 
- [NeurIPS 2024] Learning to Merge Tokens via **Decoupled Embedding** for Efficient Vision Transformers <a href="https://arxiv.org/abs/2412.10569"><img src="https://img.shields.io/badge/arXiv-2412.10569-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/movinghoon/dtem"><img src="https://img.shields.io/github/stars/movinghoon/dtem?label=dtem&logo=github&style=flat-square" height="16"></a> 
- **FovealNet**: Advancing AI-Driven Gaze Tracking Solutions for Optimized Foveated Rendering System Performance in Virtual Reality <a href="https://arxiv.org/abs/2412.10456"><img src="https://img.shields.io/badge/arXiv-2412.10456-b31b1b?logo=arxiv" height="16"></a>
- **SweetTokenizer**: Semantic-Aware Spatial-Temporal Tokenizer for Compact Visual Discretization <a href="https://arxiv.org/abs/2412.10443"><img src="https://img.shields.io/badge/arXiv-2412.10443-b31b1b?logo=arxiv" height="16"></a>
- **B-VLLM**: A Vision Large Language Model with Balanced Spatio-Temporal Tokens <a href="https://arxiv.org/abs/2412.09919"><img src="https://img.shields.io/badge/arXiv-2412.09919-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/zhuqiangLu/B-VLLM"><img src="https://img.shields.io/github/stars/zhuqiangLu/B-VLLM?label=B VLLM&logo=github&style=flat-square" height="16"></a> 
- **PVC**: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models <a href="https://arxiv.org/abs/2412.09613"><img src="https://img.shields.io/badge/arXiv-2412.09613-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/OpenGVLab/PVC"><img src="https://img.shields.io/github/stars/OpenGVLab/PVC?label=PVC&logo=github&style=flat-square" height="16"></a> 
- **Dynamic-VLM**: Simple Dynamic Visual Token Compression for VideoLLM <a href="https://arxiv.org/abs/2412.09530"><img src="https://img.shields.io/badge/arXiv-2412.09530-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/Hon-Wong/ByteVideoLLM"><img src="https://img.shields.io/github/stars/Hon-Wong/ByteVideoLLM?label=ByteVideoLLM&logo=github&style=flat-square" height="16"></a> 
- **Lyra**: An Efficient and Speech-Centric Framework for Omni-Cognition <a href="https://arxiv.org/abs/2412.09501"><img src="https://img.shields.io/badge/arXiv-2412.09501-b31b1b?logo=arxiv" height="16"></a>
- **LLaVA-Zip**: Adaptive Visual Token Compression with Intrinsic Image Information <a href="https://arxiv.org/abs/2412.08771"><img src="https://img.shields.io/badge/arXiv-2412.08771-b31b1b?logo=arxiv" height="16"></a>
- **TRIM**: Token Reduction and Inference Modeling for Cost-Effective Language Generation <a href="https://arxiv.org/abs/2412.07682"><img src="https://img.shields.io/badge/arXiv-2412.07682-b31b1b?logo=arxiv" height="16"></a>
- **RADIO Amplified**: Improved Baselines for Agglomerative Vision Foundation Models <a href="https://arxiv.org/abs/2412.07679"><img src="https://img.shields.io/badge/arXiv-2412.07679-b31b1b?logo=arxiv" height="16"></a>
- **SafeWatch**: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations <a href="https://arxiv.org/abs/2412.06878"><img src="https://img.shields.io/badge/arXiv-2412.06878-b31b1b?logo=arxiv" height="16"></a>
- **Pruning All-Rounder**: Rethinking and Improving Inference Efficiency for Large Vision Language Models <a href="https://arxiv.org/abs/2412.06458"><img src="https://img.shields.io/badge/arXiv-2412.06458-b31b1b?logo=arxiv" height="16"></a>
- **iLLaVA**: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models <a href="https://arxiv.org/abs/2412.06263"><img src="https://img.shields.io/badge/arXiv-2412.06263-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/hulianyuyy/iLLaVA"><img src="https://img.shields.io/github/stars/hulianyuyy/iLLaVA?label=iLLaVA&logo=github&style=flat-square" height="16"></a> 
- **[CLS] Token** Tells Everything Needed for Training-free Efficient MLLMs <a href="https://arxiv.org/abs/2412.05819"><img src="https://img.shields.io/badge/arXiv-2412.05819-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/THU-MIG/VTC-CLS"><img src="https://img.shields.io/github/stars/THU-MIG/VTC-CLS?label=VTC CLS&logo=github&style=flat-square" height="16"></a> 
- **VisionZip**: Longer is Better but Not Necessary in Vision Language Models <a href="https://arxiv.org/abs/2412.04467"><img src="https://img.shields.io/badge/arXiv-2412.04467-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/dvlab-research/VisionZip"><img src="https://img.shields.io/github/stars/dvlab-research/VisionZip?label=VisionZip&logo=github&style=flat-square" height="16"></a> 
- **p-MoD**: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay <a href="https://arxiv.org/abs/2412.04449"><img src="https://img.shields.io/badge/arXiv-2412.04449-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/MCG-NJU/p-MoD"><img src="https://img.shields.io/github/stars/MCG-NJU/p-MoD?label=p MoD&logo=github&style=flat-square" height="16"></a> 
- **A Stitch in Time Saves Nine**: Small VLM is a Precise Guidance for Accelerating Large VLMs <a href="https://arxiv.org/abs/2412.03324"><img src="https://img.shields.io/badge/arXiv-2412.03324-b31b1b?logo=arxiv" height="16"></a>
- **AIM**: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning <a href="https://arxiv.org/abs/2412.03248"><img src="https://img.shields.io/badge/arXiv-2412.03248-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/LaVi-Lab/AIM"><img src="https://img.shields.io/github/stars/LaVi-Lab/AIM?label=AIM&logo=github&style=flat-square" height="16"></a> 
- **3D Representation in 512-Byte**: Variational Tokenizer is the Key for Autoregressive 3D Generation <a href="https://arxiv.org/abs/2412.02202"><img src="https://img.shields.io/badge/arXiv-2412.02202-b31b1b?logo=arxiv" height="16"></a>
- **[CLS] Attention** is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster <a href="https://arxiv.org/abs/2412.01818"><img src="https://img.shields.io/badge/arXiv-2412.01818-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/Theia-4869/FasterVLM"><img src="https://img.shields.io/github/stars/Theia-4869/FasterVLM?label=FasterVLM&logo=github&style=flat-square" height="16"></a> 
- **Negative Token Merging**: Image-based Adversarial Feature Guidance <a href="https://arxiv.org/abs/2412.01339"><img src="https://img.shields.io/badge/arXiv-2412.01339-b31b1b?logo=arxiv" height="16"></a>  <a href="https://negtome.github.io/"><img src="https://img.shields.io/badge/Project-negtome-yellow" height="16"></a> 
- **Token Cropr**: Faster ViTs for Quite a Few Tasks <a href="https://arxiv.org/abs/2412.00965"><img src="https://img.shields.io/badge/arXiv-2412.00965-b31b1b?logo=arxiv" height="16"></a>
- Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction <a href="https://arxiv.org/abs/2412.00556"><img src="https://img.shields.io/badge/arXiv-2412.00556-b31b1b?logo=arxiv" height="16"></a>
- **ATP-LLaVA**: Adaptive Token Pruning for Large Vision Language Models <a href="https://arxiv.org/abs/2412.00447"><img src="https://img.shields.io/badge/arXiv-2412.00447-b31b1b?logo=arxiv" height="16"></a>  <a href="https://yxxxb.github.io/ATP-LLaVA-page/"><img src="https://img.shields.io/badge/Project-ATP LLaVA-yellow" height="16"></a>
- Accelerating Multimodal Large Language Models via **Dynamic Visual-Token Exit** and the Empirical Findings <a href="https://arxiv.org/abs/2411.19628"><img src="https://img.shields.io/badge/arXiv-2411.19628-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/DoubtedSteam/DyVTE"><img src="https://img.shields.io/github/stars/DoubtedSteam/DyVTE?label=DyVTE&logo=github&style=flat-square" height="16"></a>
- **TimeMarker**: A Versatile Video-LLM for Long and Short Video Understanding with Superior Temporal Localization Ability  <a href="https://arxiv.org/abs/2411.18211"><img src="https://img.shields.io/badge/arXiv-2411.18211-b31b1b?logo=arxiv" height="16"></a>
- **Training Noise Token Pruning** <a href="https://arxiv.org/abs/2411.18092"><img src="https://img.shields.io/badge/arXiv-2411.18092-b31b1b?logo=arxiv" height="16"></a>
- Efficient Multi-modal Large Language Models via **Visual Token Grouping** <a href="https://arxiv.org/abs/2411.17773"><img src="https://img.shields.io/badge/arXiv-2411.17773-b31b1b?logo=arxiv" height="16"></a>
- Rethinking Token Reduction in MLLMs: Towards a **Unified Paradigm for Training-Free Acceleration** <a href="https://arxiv.org/abs/2411.17686"><img src="https://img.shields.io/badge/arXiv-2411.17686-b31b1b?logo=arxiv" height="16"></a>  <a href="https://ficoco-accelerate.github.io/"><img src="https://img.shields.io/badge/Project-ficoco-yellow" height="16"></a> 
- **Attamba**: Attending To Multi-Token States <a href="https://arxiv.org/abs/2411.17685"><img src="https://img.shields.io/badge/arXiv-2411.17685-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPSW 2024] **ShowUI**: One Vision-Language-Action Model for GUI Visual Agent <a href="https://arxiv.org/abs/2411.17465"><img src="https://img.shields.io/badge/arXiv-2411.17465-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/showlab/ShowUI"><img src="https://img.shields.io/github/stars/showlab/ShowUI?label=ShowUI&logo=github&style=flat-square" height="16"></a> 
- **Importance-based Token Merging** for Diffusion Models <a href="https://arxiv.org/abs/2411.16720"><img src="https://img.shields.io/badge/arXiv-2411.16720-b31b1b?logo=arxiv" height="16"></a>
- Enhancing Instruction-Following Capability of Visual-Language Models by **Reducing Image Redundancy** <a href="https://arxiv.org/abs/2411.15453"><img src="https://img.shields.io/badge/arXiv-2411.15453-b31b1b?logo=arxiv" height="16"></a>
- **freePruner**: A Training-free Approach for Large Multimodal Model Acceleration <a href="https://arxiv.org/abs/2411.15446"><img src="https://img.shields.io/badge/arXiv-2411.15446-b31b1b?logo=arxiv" height="16"></a>
- Efficient Online Inference of Vision Transformers by **Training-Free Tokenization** <a href="https://arxiv.org/abs/2411.15397"><img src="https://img.shields.io/badge/arXiv-2411.15397-b31b1b?logo=arxiv" height="16"></a>
- **DyCoke**: Dynamic Compression of Tokens for Fast Video Large Language Models <a href="https://arxiv.org/abs/2411.15024"><img src="https://img.shields.io/badge/arXiv-2411.15024-b31b1b?logo=arxiv" height="16"></a> 
- **LLaVA-MR**: Large Language-and-Vision Assistant for Video Moment Retrieval <a href="https://arxiv.org/abs/2411.14505"><img src="https://img.shields.io/badge/arXiv-2411.14505-b31b1b?logo=arxiv" height="16"></a>
- **FOCUS**: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification <a href="https://arxiv.org/abs/2411.14743"><img src="https://img.shields.io/badge/arXiv-2411.14743-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/dddavid4real/FOCUS"><img src="https://img.shields.io/github/stars/dddavid4real/FOCUS?label=FOCUS&logo=github&style=flat-square" height="16"></a> 
- Beyond Training: **Dynamic Token Merging** for Zero-Shot Video Understanding <a href="https://arxiv.org/abs/2411.14401"><img src="https://img.shields.io/badge/arXiv-2411.14401-b31b1b?logo=arxiv" height="16"></a>
- **FocusLLaVA**: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression <a href="https://arxiv.org/abs/2411.14228"><img src="https://img.shields.io/badge/arXiv-2411.14228-b31b1b?logo=arxiv" height="16"></a>
- **FoPru**: Focal Pruning for Efficient Large Vision-Language Models <a href="https://arxiv.org/abs/2411.14164"><img src="https://img.shields.io/badge/arXiv-2411.14164-b31b1b?logo=arxiv" height="16"></a>
- **Principles of Visual Tokens** for Efficient Video Understanding <a href="https://arxiv.org/abs/2411.13626"><img src="https://img.shields.io/badge/arXiv-2411.13626-b31b1b?logo=arxiv" height="16"></a>
- **LaVida Drive**: Vision-Text Interaction VLM for Autonomous Driving with Token Selection, Recovery and Enhancement <a href="https://arxiv.org/abs/2411.12980"><img src="https://img.shields.io/badge/arXiv-2411.12980-b31b1b?logo=arxiv" height="16"></a>
- **TS-LLaVA**: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models <a href="https://arxiv.org/abs/2411.11066"><img src="https://img.shields.io/badge/arXiv-2411.11066-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/tingyu215/TS-LLaVA"><img src="https://img.shields.io/github/stars/tingyu215/TS-LLaVA?label=TS LLaVA&logo=github&style=flat-square" height="16"></a> 
- **Multi-Stage Vision Token Dropping**: Towards Efficient Multimodal Large Language Model <a href="https://arxiv.org/abs/2411.10803"><img src="https://img.shields.io/badge/arXiv-2411.10803-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/liuting20/MustDrop"><img src="https://img.shields.io/github/stars/liuting20/MustDrop?label=MustDrop&logo=github&style=flat-square" height="16"></a> 
- **Multidimensional Byte Pair Encoding**: Shortened Sequences for Improved Visual Data Generation <a href="https://arxiv.org/abs/2411.10281"><img src="https://img.shields.io/badge/arXiv-2411.10281-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2024] Token Merging for **Training-Free Semantic Binding** in Text-to-Image Synthesis <a href="https://arxiv.org/abs/2411.07132"><img src="https://img.shields.io/badge/arXiv-2411.07132-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/hutaihang/ToMe"><img src="https://img.shields.io/github/stars/hutaihang/ToMe?label=ToMe&logo=github&style=flat-square" height="16"></a> 
- [NeurIPS 2024 Spotlight] Donâ€™t Look Twice: Faster Video Transformers with **Run-Length Tokenization** <a href="https://arxiv.org/abs/2411.05222"><img src="https://img.shields.io/badge/arXiv-2411.05222-b31b1b?logo=arxiv" height="16"></a> <a href="https://rccchoudhury.github.io/projects/rlt/"><img src="https://img.shields.io/badge/Project-rlt-yellow" height="16"></a> 
- Inference Optimal VLMs Need Only **One Visual Token but Larger Models** <a href="https://arxiv.org/abs/2411.03312"><img src="https://img.shields.io/badge/arXiv-2411.03312-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/locuslab/llava-token-compression"><img src="https://img.shields.io/github/stars/locuslab/llava-token-compression?label=token compression&logo=github&style=flat-square" height="16"></a> 
- **PPLLaVA**: Varied Video Sequence Understanding With Prompt Guidance <a href="https://arxiv.org/abs/2411.02327"><img src="https://img.shields.io/badge/arXiv-2411.02327-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/farewellthree/PPLLaVA"><img src="https://img.shields.io/github/stars/farewellthree/PPLLaVA?label=PPLLaVA&logo=github&style=flat-square" height="16"></a> 
- [NeurIPS 2024] **EDT**: An Efficient Diffusion Transformer Framework Inspired by Human-like Sketching <a href="https://arxiv.org/abs/2410.23788"><img src="https://img.shields.io/badge/arXiv-2410.23788-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/xinwangChen/EDT"><img src="https://img.shields.io/github/stars/xinwangChen/EDT?label=EDT&logo=github&style=flat-square" height="16"></a> 
- [NeurIPS 2024] **Video Token Merging** for Long-form Video Understanding <a href="https://arxiv.org/abs/2410.23782"><img src="https://img.shields.io/badge/arXiv-2410.23782-b31b1b?logo=arxiv" height="16"></a>
- **MrT5**: Dynamic Token Merging for Efficient Byte-level Language Models <a href="https://arxiv.org/abs/2410.20771"><img src="https://img.shields.io/badge/arXiv-2410.20771-b31b1b?logo=arxiv" height="16"></a>
- Rethinking **Visual Dependency in Long-Context Reasoning** for Large Vision-Language Models <a href="https://arxiv.org/abs/2410.19732"><img src="https://img.shields.io/badge/arXiv-2410.19732-b31b1b?logo=arxiv" height="16"></a>
- **LongVU**: Spatiotemporal Adaptive Compression for Long Video-Language Understanding <a href="https://arxiv.org/abs/2410.17434"><img src="https://img.shields.io/badge/arXiv-2410.17434-b31b1b?logo=arxiv" height="16"></a>  <a href="https://vision-cair.github.io/LongVU"><img src="https://img.shields.io/badge/Project-LongVU-yellow" height="16"></a> 
- **PyramidDrop**: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction <a href="https://arxiv.org/abs/2410.17247"><img src="https://img.shields.io/badge/arXiv-2410.17247-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/Cooperx521/PyramidDrop"><img src="https://img.shields.io/github/stars/Cooperx521/PyramidDrop?label=PyramidDrop&logo=github&style=flat-square" height="16"></a>
- **xGen-MM-Vid (BLIP-3-Video)**: You Only Need 32 Tokens to Represent a Video Even in VLMs <a href="https://arxiv.org/abs/2410.16267"><img src="https://img.shields.io/badge/arXiv-2410.16267-b31b1b?logo=arxiv" height="16"></a>  <a href="https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html"><img src="https://img.shields.io/badge/Project-xGen MM Vid-yellow" height="16"></a>
- **CAGE**: Causal Attention Enables Data-Efficient Generalizable Robotic Manipulation  <a href="https://arxiv.org/abs/2410.14974"><img src="https://img.shields.io/badge/arXiv-2410.14974-b31b1b?logo=arxiv" height="16"></a>
- Is Less More? Exploring **Token Condensation** as Training-free Adaptation for CLIP <a href="https://arxiv.org/abs/2410.14729"><img src="https://img.shields.io/badge/arXiv-2410.14729-b31b1b?logo=arxiv" height="16"></a>
- [EMNLP 2024] Rethinking Token Reduction for **State Space Models** <a href="https://arxiv.org/abs/2410.14725"><img src="https://img.shields.io/badge/arXiv-2410.14725-b31b1b?logo=arxiv" height="16"></a>
- Efficient Vision-Language Models by Summarizing Visual Tokens into **Compact Registers** <a href="https://arxiv.org/abs/2410.14072"><img src="https://img.shields.io/badge/arXiv-2410.14072-b31b1b?logo=arxiv" height="16"></a>
- **Long-LRM**: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats <a href="https://arxiv.org/abs/2410.12781"><img src="https://img.shields.io/badge/arXiv-2410.12781-b31b1b?logo=arxiv" height="16"></a>
- **VidCompress**: Memory-Enhanced Temporal Compression for Video Understanding in Large Language Models <a href="https://arxiv.org/abs/2410.11417"><img src="https://img.shields.io/badge/arXiv-2410.11417-b31b1b?logo=arxiv" height="16"></a>
- **big.LITTLE Vision Transformer** for Efficient Visual Recognition <a href="https://arxiv.org/abs/2410.10267"><img src="https://img.shields.io/badge/arXiv-2410.10267-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPSW 2024] Token Pruning using a **Lightweight Background Aware Vision Transformer** <a href="https://arxiv.org/abs/2410.09324"><img src="https://img.shields.io/badge/arXiv-2410.09324-b31b1b?logo=arxiv" height="16"></a>
- **ZipVL**: Efficient Large Vision-Language Models with Dynamic Token Sparsification <a href="https://arxiv.org/abs/2410.08584"><img src="https://img.shields.io/badge/arXiv-2410.08584-b31b1b?logo=arxiv" height="16"></a>
- **PAR**: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models <a href="https://arxiv.org/abs/2410.07278"><img src="https://img.shields.io/badge/arXiv-2410.07278-b31b1b?logo=arxiv" height="16"></a>
- **Rodimus***: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions <a href="https://arxiv.org/abs/2410.06577"><img src="https://img.shields.io/badge/arXiv-2410.06577-b31b1b?logo=arxiv" height="16"></a>
- **Treat Visual Tokens as Text?** But Your MLLM Only Needs Fewer Efforts to See <a href="https://arxiv.org/abs/2410.06169"><img src="https://img.shields.io/badge/arXiv-2410.06169-b31b1b?logo=arxiv" height="16"></a>
- **TextHawk2**: A Large Vision-Language Model Excels in Bilingual OCR and Grounding with 16x Fewer Tokens <a href="https://arxiv.org/abs/2410.05261"><img src="https://img.shields.io/badge/arXiv-2410.05261-b31b1b?logo=arxiv" height="16"></a>
- **SparseVLM**: Visual Token Sparsification for Efficient Vision-Language Model Inference  <a href="https://arxiv.org/abs/2410.04417"><img src="https://img.shields.io/badge/arXiv-2410.04417-b31b1b?logo=arxiv" height="16"></a>
- [EMNLP 2024 Findings] **From Reading to Compressing**: Exploring the Multi-document Reader for Prompt Compression <a href="https://arxiv.org/abs/2410.04139"><img src="https://img.shields.io/badge/arXiv-2410.04139-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/eunseongc/R2C"><img src="https://img.shields.io/github/stars/eunseongc/R2C?label=R2C&logo=github&style=flat-square" height="16"></a> 
- [ICLR 2025] **Dynamic Diffusion Transformer** <a href="https://arxiv.org/abs/2410.03456"><img src="https://img.shields.io/badge/arXiv-2410.03456-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/NUS-HPC-AI-Lab/Dynamic-Diffusion-Transformer"><img src="https://img.shields.io/github/stars/NUS-HPC-AI-Lab/Dynamic-Diffusion-Transformer?label=DDT&logo=github&style=flat-square" height="16"></a> 
- **AuroraCap**: Efficient, Performant Video Detailed Captioning and a New Benchmark <a href="https://arxiv.org/abs/2410.03051"><img src="https://img.shields.io/badge/arXiv-2410.03051-b31b1b?logo=arxiv" height="16"></a>  <a href="https://rese1f.github.io/aurora-web/"><img src="https://img.shields.io/badge/Project-aurora-yellow" height="16"></a> 
- [EMNLP 2024] **FastAdaSP**: Multitask-Adapted Efficient Inference for Large Speech Language Model <a href="https://arxiv.org/abs/2410.03007"><img src="https://img.shields.io/badge/arXiv-2410.03007-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/yichen14/FastAdaSP"><img src="https://img.shields.io/github/stars/yichen14/FastAdaSP?label=FastAdaSP&logo=github&style=flat-square" height="16"></a> 
- **AVG-LLaVA**: A Large Multimodal Model with Adaptive Visual Granularity <a href="https://arxiv.org/abs/2410.02745"><img src="https://img.shields.io/badge/arXiv-2410.02745-b31b1b?logo=arxiv" height="16"></a>
- **Cut the Crap**: An Economical Communication Pipeline for LLM-based Multi-Agent Systems  <a href="https://arxiv.org/abs/2410.02506"><img src="https://img.shields.io/badge/arXiv-2410.02506-b31b1b?logo=arxiv" height="16"></a>
- **Locret**: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads <a href="https://arxiv.org/abs/2410.01805"><img src="https://img.shields.io/badge/arXiv-2410.01805-b31b1b?logo=arxiv" height="16"></a>
- **VMAD**: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection <a href="https://arxiv.org/abs/2409.20146"><img src="https://img.shields.io/badge/arXiv-2409.20146-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2024] **One Token to Seg Them All**: Language Instructed Reasoning Segmentation in Videos <a href="https://arxiv.org/abs/2409.19603"><img src="https://img.shields.io/badge/arXiv-2409.19603-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/showlab/VideoLISA"><img src="https://img.shields.io/github/stars/showlab/VideoLISA?label=VideoLISA&logo=github&style=flat-square" height="16"></a> 
- [NeurIPS 2024] Exploring Token Pruning in **Vision State Space Models** <a href="https://arxiv.org/abs/2409.18962"><img src="https://img.shields.io/badge/arXiv-2409.18962-b31b1b?logo=arxiv" height="16"></a>
- **Token Caching** for Diffusion Transformer Acceleration <a href="https://arxiv.org/abs/2409.18523"><img src="https://img.shields.io/badge/arXiv-2409.18523-b31b1b?logo=arxiv" height="16"></a>
- **Discovering the Gems in Early Layers**: Accelerating Long-Context LLMs with 1000x Input Token Reduction <a href="https://arxiv.org/abs/2409.17422"><img src="https://img.shields.io/badge/arXiv-2409.17422-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/SalesforceAIResearch/GemFilter"><img src="https://img.shields.io/github/stars/SalesforceAIResearch/GemFilter?label=GemFilter&logo=github&style=flat-square" height="16"></a> 
- [WACV 2025] **Patch Ranking**: Token Pruning as Ranking Prediction for Efficient CLIP <a href="https://arxiv.org/abs/2409.14607"><img src="https://img.shields.io/badge/arXiv-2409.14607-b31b1b?logo=arxiv" height="16"></a>
- **Video-XL**: Extra-Long Vision Language Model for Hour-Scale Video Understanding <a href="https://arxiv.org/abs/2409.14485"><img src="https://img.shields.io/badge/arXiv-2409.14485-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **Agglomerative Token Clustering** <a href="https://arxiv.org/abs/2409.11923"><img src="https://img.shields.io/badge/arXiv-2409.11923-b31b1b?logo=arxiv" height="16"></a>  <a href="https://vap.aau.dk/atc/"><img src="https://img.shields.io/badge/Project-atc-yellow" height="16"></a> 
- **Video Token Sparsification** for Efficient Multimodal LLMs in Autonomous Driving <a href="https://arxiv.org/abs/2409.11182"><img src="https://img.shields.io/badge/arXiv-2409.11182-b31b1b?logo=arxiv" height="16"></a>
- [COLING 2025] **Less is More**: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs <a href="https://arxiv.org/abs/2409.10994"><img src="https://img.shields.io/badge/arXiv-2409.10994-b31b1b?logo=arxiv" height="16"></a>
- **CSKV**: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios  <a href="https://arxiv.org/abs/2409.10593"><img src="https://img.shields.io/badge/arXiv-2409.10593-b31b1b?logo=arxiv" height="16"></a>
- [AAAI 2025] **Fit and Prune**: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models <a href="https://arxiv.org/abs/2409.10197"><img src="https://img.shields.io/badge/arXiv-2409.10197-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/ywh187/FitPrune"><img src="https://img.shields.io/github/stars/ywh187/FitPrune?label=FitPrune&logo=github&style=flat-square" height="16"></a> 
- [ECCVW 2024] **Famba-V**: Fast Vision Mamba with Cross-Layer Token Fusion <a href="https://arxiv.org/abs/2409.09808"><img src="https://img.shields.io/badge/arXiv-2409.09808-b31b1b?logo=arxiv" height="16"></a>
- **TG-LLaVA**: Text Guided LLaVA via Learnable Latent Embeddings <a href="https://arxiv.org/abs/2409.09564"><img src="https://img.shields.io/badge/arXiv-2409.09564-b31b1b?logo=arxiv" height="16"></a>
- [WACV 2025] **VLTP**: Vision-Language Guided Token Pruning for Task-Oriented Segmentation  <a href="https://arxiv.org/abs/2409.08464"><img src="https://img.shields.io/badge/arXiv-2409.08464-b31b1b?logo=arxiv" height="16"></a>
- Enhancing Long Video Understanding via **Hierarchical Event-Based Memory** <a href="https://arxiv.org/abs/2409.06299"><img src="https://img.shields.io/badge/arXiv-2409.06299-b31b1b?logo=arxiv" height="16"></a>
- **Qihoo-T2X**: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Any-Task <a href="https://arxiv.org/abs/2409.04005"><img src="https://img.shields.io/badge/arXiv-2409.04005-b31b1b?logo=arxiv" height="16"></a>  <a href="https://360cvgroup.github.io/Qihoo-T2X/"><img src="https://img.shields.io/badge/Project-Qihoo T2X-yellow" height="16"></a> 
- **mPLUG-DocOwl2**: High-resolution Compressing for OCR-free Multi-page Document Understanding <a href="https://arxiv.org/abs/2409.03420"><img src="https://img.shields.io/badge/arXiv-2409.03420-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2"><img src="https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?label=DocOwl2&logo=github&style=flat-square" height="16"></a> 
- **TC-LLaVA**: Rethinking the Transfer from Image to Video Understanding with Temporal Consideration <a href="https://arxiv.org/abs/2409.03206"><img src="https://img.shields.io/badge/arXiv-2409.03206-b31b1b?logo=arxiv" height="16"></a>
- **LongLLaVA**: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture <a href="https://arxiv.org/abs/2409.02889"><img src="https://img.shields.io/badge/arXiv-2409.02889-b31b1b?logo=arxiv" height="16"></a>
- [AAAI 2025] **Recoverable Compression**: A Multimodal Vision Token Recovery Mechanism Guided by Text Information <a href="https://arxiv.org/abs/2409.01179"><img src="https://img.shields.io/badge/arXiv-2409.01179-b31b1b?logo=arxiv" height="16"></a>
- **Balancing Performance and Efficiency**: A Multimodal Large Language Model Pruning Method based on Image Text Interaction <a href="https://arxiv.org/abs/2409.01162"><img src="https://img.shields.io/badge/arXiv-2409.01162-b31b1b?logo=arxiv" height="16"></a>
- [ICLR 2025] **TempMe**: Video Temporal Token Merging for Efficient Text-Video Retrieval <a href="https://arxiv.org/abs/2409.01156"><img src="https://img.shields.io/badge/arXiv-2409.01156-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] Make Your ViT-based **Multi-view 3D Detectors** Faster via Token Compression <a href="https://arxiv.org/abs/2409.00633"><img src="https://img.shields.io/badge/arXiv-2409.00633-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/DYZhang09/ToC3D"><img src="https://img.shields.io/github/stars/DYZhang09/ToC3D?label=ToC3D&logo=github&style=flat-square" height="16"></a> 
- **Vote&Mix**: Plug-and-Play Token Reduction for Efficient Vision Transformer <a href="https://arxiv.org/abs/2408.17062"><img src="https://img.shields.io/badge/arXiv-2408.17062-b31b1b?logo=arxiv" height="16"></a>
- **TReX**- Reusing Vision Transformer's Attention for Efficient Xbar-based Computing <a href="https://arxiv.org/abs/2408.12742"><img src="https://img.shields.io/badge/arXiv-2408.12742-b31b1b?logo=arxiv" height="16"></a>
- **AT-SNN**: Adaptive Tokens for Vision Transformer on Spiking Neural Network <a href="https://arxiv.org/abs/2408.12293"><img src="https://img.shields.io/badge/arXiv-2408.12293-b31b1b?logo=arxiv" height="16"></a>
- **Practical token pruning** for foundation models in few-shot conversational virtual assistant systems <a href="https://arxiv.org/abs/2408.11799"><img src="https://img.shields.io/badge/arXiv-2408.11799-b31b1b?logo=arxiv" height="16"></a>
- [AAAI 2025] **HiRED**: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments <a href="https://arxiv.org/abs/2408.10945"><img src="https://img.shields.io/badge/arXiv-2408.10945-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/hasanar1f/HiRED"><img src="https://img.shields.io/github/stars/hasanar1f/HiRED?label=HiRED&logo=github&style=flat-square" height="16"></a> 
- **Dynamic and Compressive Adaptation of Transformers** From Images to Videos <a href="https://arxiv.org/abs/2408.06840"><img src="https://img.shields.io/badge/arXiv-2408.06840-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **Token Compensator**: Altering Inference Cost of Vision Transformer without Re-Tuning <a href="https://arxiv.org/abs/2408.06798"><img src="https://img.shields.io/badge/arXiv-2408.06798-b31b1b?logo=arxiv"   height="16"></a> <a href="https://github.com/JieShibo/ToCom"><img src="https://img.shields.io/github/stars/JieShibo/ToCom?label=ToCom&logo=github&style=flat-square" height="16"></a>
- [ACMMM 2024] **ReToMe-VA**: Recursive Token Merging for Video Diffusion-based Unrestricted Adversarial Attack <a href="https://arxiv.org/abs/2408.05479"><img src="https://img.shields.io/badge/arXiv-2408.05479-b31b1b?logo=arxiv" height="16"></a>
- **A2SF**: Accumulative Attention Scoring with Forgetting Factor for Token Pruning in Transformer Decoder <a href="https://arxiv.org/abs/2407.20485"><img src="https://img.shields.io/badge/arXiv-2407.20485-b31b1b?logo=arxiv" height="16"></a>
- **Exploring The Neural Burden In Pruned Models**: An Insight Inspired By Neuroscience <a href="https://arxiv.org/abs/2407.16716"><img src="https://img.shields.io/badge/arXiv-2407.16716-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2024] **Data Mixture Inference**: What do BPE Tokenizers Reveal about their Training Data? <a href="https://arxiv.org/abs/2407.16607"><img src="https://img.shields.io/badge/arXiv-2407.16607-b31b1b?logo=arxiv" height="16"></a>
- **SlowFast-LLaVA**: A Strong Training-Free Baseline for Video Large Language Models <a href="https://arxiv.org/abs/2407.15841"><img src="https://img.shields.io/badge/arXiv-2407.15841-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/apple/ml-slowfast-llava"><img src="https://img.shields.io/github/stars/apple/ml-slowfast-llava?label=slowfast llava&logo=github&style=flat-square" height="16"></a> 
- Efficient Visual Transformer by **Learnable Token Merging** <a href="https://arxiv.org/abs/2407.15219"><img src="https://img.shields.io/badge/arXiv-2407.15219-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/Statistical-Deep-Learning/LTM"><img src="https://img.shields.io/github/stars/Statistical-Deep-Learning/LTM?label=LTM&logo=github&style=flat-square" height="16"></a> 
- **Token-level Correlation-guided Compression** for Efficient Multimodal Document Understanding <a href="https://arxiv.org/abs/2407.14439"><img src="https://img.shields.io/badge/arXiv-2407.14439-b31b1b?logo=arxiv" height="16"></a>
- **LazyLLM**: Dynamic Token Pruning for Efficient Long Context LLM Inference <a href="https://arxiv.org/abs/2407.14057"><img src="https://img.shields.io/badge/arXiv-2407.14057-b31b1b?logo=arxiv" height="16"></a>
- **Pose-guided Multi-task Video Transformer** for Driver Action Recognition <a href="https://arxiv.org/abs/2407.13750"><img src="https://img.shields.io/badge/arXiv-2407.13750-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **LookupViT**: Compressing Visual Information to a Limited Number of Tokens <a href="https://arxiv.org/abs/2407.12753"><img src="https://img.shields.io/badge/arXiv-2407.12753-b31b1b?logo=arxiv" height="16"></a>
- [TPAMI 2024] **TCFormer**: Visual Recognition via Token Clustering Transformer <a href="https://arxiv.org/abs/2407.11321"><img src="https://img.shields.io/badge/arXiv-2407.11321-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/zengwang430521/TCFormer"><img src="https://img.shields.io/github/stars/zengwang430521/TCFormer?label=TCFormer&logo=github&style=flat-square" height="16"></a> 
- [ECCV 2024] **Turbo**: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models <a href="https://arxiv.org/abs/2407.11717"><img src="https://img.shields.io/badge/arXiv-2407.11717-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **GTPT**: Group-based Token Pruning Transformer for Efficient Human Pose Estimation <a href="https://arxiv.org/abs/2407.10756"><img src="https://img.shields.io/badge/arXiv-2407.10756-b31b1b?logo=arxiv" height="16"></a>
- [Interpeech] **LiteFocus**: Accelerated Diffusion Inference for Long Audio Synthesis <a href="https://arxiv.org/abs/2407.10468"><img src="https://img.shields.io/badge/arXiv-2407.10468-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/Yuanshi9815/LiteFocus"><img src="https://img.shields.io/github/stars/Yuanshi9815/LiteFocus?label=LiteFocus&logo=github&style=flat-square" height="16"></a>
- [ICMLW 2024] **Characterizing Prompt Compression Methods** for Long Context Inference  <a href="https://arxiv.org/abs/2407.08892"><img src="https://img.shields.io/badge/arXiv-2407.08892-b31b1b?logo=arxiv" height="16"></a>
- **HiRes-LLaVA**: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models <a href="https://arxiv.org/abs/2407.08706"><img src="https://img.shields.io/badge/arXiv-2407.08706-b31b1b?logo=arxiv" height="16"></a>
- **Pruning One More Token is Enough**: Leveraging Latency-Workload Non-Linearities for Vision Transformers on the Edge <a href="https://arxiv.org/abs/2407.05941"><img src="https://img.shields.io/badge/arXiv-2407.05941-b31b1b?logo=arxiv" height="16"></a>
- **PRANCE**: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference <a href="https://arxiv.org/abs/2407.05010"><img src="https://img.shields.io/badge/arXiv-2407.05010-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/ChildTang/PRANCE"><img src="https://img.shields.io/github/stars/ChildTang/PRANCE?label=PRANCE&logo=github&style=flat-square" height="16"></a> 
- **ALPINE**: An Adaptive Language-Agnostic Pruning Method for Language Models for Code <a href="https://arxiv.org/abs/2407.04147"><img src="https://img.shields.io/badge/arXiv-2407.04147-b31b1b?logo=arxiv" height="16"></a>
- **TokenPacker**: Efficient Visual Projector for Multimodal LLM <a href="https://arxiv.org/abs/2407.02392"><img src="https://img.shields.io/badge/arXiv-2407.02392-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/CircleRadon/TokenPacker"><img src="https://img.shields.io/github/stars/CircleRadon/TokenPacker?label=TokenPacker&logo=github&style=flat-square" height="16"></a> 
- [ECCV 2024] **LPViT**: Low-Power Semi-structured Pruning for Vision Transformers <a href="https://arxiv.org/abs/2407.02068"><img src="https://img.shields.io/badge/arXiv-2407.02068-b31b1b?logo=arxiv" height="16"></a>
- [ACL 2024 Findings] **Concise and Precise Context Compression** for Tool-Using Language Models <a href="https://arxiv.org/abs/2407.02043"><img src="https://img.shields.io/badge/arXiv-2407.02043-b31b1b?logo=arxiv" height="16"></a>
- **DiffIR2VR-Zero**: Zero-Shot Video Restoration with Diffusion-based Image Restoration Models <a href="https://arxiv.org/abs/2407.01519"><img src="https://img.shields.io/badge/arXiv-2407.01519-b31b1b?logo=arxiv" height="16"></a>  <a href="https://jimmycv07.github.io/DiffIR2VR_web/"><img src="https://img.shields.io/badge/Project-DiffIR2VR-yellow" height="16"></a> 
- [ICASSP 2023] **Papez**: Resource-Efficient Speech Separation with Auditory Working Memory <a href="https://arxiv.org/abs/2407.00888"><img src="https://img.shields.io/badge/arXiv-2407.00888-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/snuhcs/Papez"><img src="https://img.shields.io/github/stars/snuhcs/Papez?label=Papez&logo=github&style=flat-square" height="16"></a> 
- [NeurIPS 2024] Efficient Large Multi-modal Models via **Visual Context Compression** <a href="https://arxiv.org/abs/2406.20092"><img src="https://img.shields.io/badge/arXiv-2406.20092-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/Beckschen/LLaVolta"><img src="https://img.shields.io/github/stars/Beckschen/LLaVolta?label=LLaVolta&logo=github&style=flat-square" height="16"></a> 
- [AAAI 2025] **DocKylin**: A Large Multimodal Model for Visual Document Understanding with Efficient Visual Slimming <a href="https://arxiv.org/abs/2406.19101"><img src="https://img.shields.io/badge/arXiv-2406.19101-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] **ScanFormer**: Referring Expression Comprehension by Iteratively Scanning <a href="https://arxiv.org/abs/2406.18048"><img src="https://img.shields.io/badge/arXiv-2406.18048-b31b1b?logo=arxiv" height="16"></a>
- [ICLR 2025] **Dynamic Discriminative Operations** for Efficient Generative Inference of Large Language Models <a href="https://arxiv.org/abs/2406.13035"><img src="https://img.shields.io/badge/arXiv-2406.13035-b31b1b?logo=arxiv" height="16"></a>
- [EMNLP 2024] **Bridging Local Details and Global Context** in Text-Attributed Graphs <a href="https://arxiv.org/abs/2406.12608"><img src="https://img.shields.io/badge/arXiv-2406.12608-b31b1b?logo=arxiv" height="16"></a>
- [EMNLP 2024] **Attention Score is not All You Need** for Token Importance Indicator in KV Cache Reduction: Value Also Matters <a href="https://arxiv.org/abs/2406.12335"><img src="https://img.shields.io/badge/arXiv-2406.12335-b31b1b?logo=arxiv" height="16"></a>
- **VoCo-LLaMA**: Towards Vision Compression with Large Language Models <a href="https://arxiv.org/abs/2406.12275"><img src="https://img.shields.io/badge/arXiv-2406.12275-b31b1b?logo=arxiv" height="16"></a> <a href="https://yxxxb.github.io/VoCo-LLaMA-page/"><img src="https://img.shields.io/badge/Project-VoCo LLaMA-yellow" height="16"></a>
- **Refiner**: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities  <a href="https://arxiv.org/abs/2406.11357"><img src="https://img.shields.io/badge/arXiv-2406.11357-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] **ALGM**: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers <a href="https://arxiv.org/abs/2406.09936"><img src="https://img.shields.io/badge/arXiv-2406.09936-b31b1b?logo=arxiv" height="16"></a>  <a href="https://tue-mps.github.io/ALGM"><img src="https://img.shields.io/badge/Project-ALGM-yellow" height="16"></a> 
- **SViTT-Ego**: A Sparse Video-Text Transformer for Egocentric Video <a href="https://arxiv.org/abs/2406.09462"><img src="https://img.shields.io/badge/arXiv-2406.09462-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2024] **COVE**: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing <a href="https://arxiv.org/abs/2406.08850"><img src="https://img.shields.io/badge/arXiv-2406.08850-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/wangjiangshan0725/COVE"><img src="https://img.shields.io/github/stars/wangjiangshan0725/COVE?label=COVE&logo=github&style=flat-square" height="16"></a> 
- [CVPRW 2024] **ToSA**: Token Selective Attention for Efficient Vision Transformers <a href="https://arxiv.org/abs/2406.08816"><img src="https://img.shields.io/badge/arXiv-2406.08816-b31b1b?logo=arxiv" height="16"></a>
- [Interspeech 2024] **FastAST**: Accelerating Audio Spectrogram Transformer via Token Merging and Cross-Model Knowledge Distillation <a href="https://arxiv.org/abs/2406.07676"><img src="https://img.shields.io/badge/arXiv-2406.07676-b31b1b?logo=arxiv" height="16"></a>
- **An Image is Worth 32 Tokens** for Reconstruction and Generation <a href="https://arxiv.org/abs/2406.07550"><img src="https://img.shields.io/badge/arXiv-2406.07550-b31b1b?logo=arxiv" height="16"></a>  <a href="https://yucornetto.github.io/projects/titok.html"><img src="https://img.shields.io/badge/Project-titok-yellow" height="16"></a> 
- [ICML 2024] **LoCoCo**: Dropping In Convolutions for Long Context Compression <a href="https://arxiv.org/abs/2406.05317"><img src="https://img.shields.io/badge/arXiv-2406.05317-b31b1b?logo=arxiv" height="16"></a>
- **REP**: Resource-Efficient Prompting for Rehearsal-Free Continual Learning  <a href="https://arxiv.org/abs/2406.04772"><img src="https://img.shields.io/badge/arXiv-2406.04772-b31b1b?logo=arxiv" height="16"></a>
- **Boosting Diffusion Model** for Spectrogram Up-sampling in Text-to-speech: An Empirical Study <a href="https://arxiv.org/abs/2406.04633"><img src="https://img.shields.io/badge/arXiv-2406.04633-b31b1b?logo=arxiv" height="16"></a>
- **DeepStack**: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs <a href="https://arxiv.org/abs/2406.04334"><img src="https://img.shields.io/badge/arXiv-2406.04334-b31b1b?logo=arxiv" height="16"></a>  <a href="https://deepstack-vl.github.io/"><img src="https://img.shields.io/badge/Project-deepstack-yellow" height="16"></a> 
- [ICML 2024] **MLIP**: Efficient Multi-Perspective Language-Image Pretraining with Exhaustive Data Utilization <a href="https://arxiv.org/abs/2406.01460"><img src="https://img.shields.io/badge/arXiv-2406.01460-b31b1b?logo=arxiv" height="16"></a>
- [EMNLP 2023 Findings] Focus on the Core: Efficient Attention via **Pruned Token Compression** for Document Classification <a href="https://arxiv.org/abs/2406.01283"><img src="https://img.shields.io/badge/arXiv-2406.01283-b31b1b?logo=arxiv" height="16"></a>
- **DeCo**: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models <a href="https://arxiv.org/abs/2405.20985"><img src="https://img.shields.io/badge/arXiv-2405.20985-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2024] **Matryoshka Query Transformer** for Large Vision-Language Models <a href="https://arxiv.org/abs/2405.19315"><img src="https://img.shields.io/badge/arXiv-2405.19315-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/gordonhu608/MQT-LLaVA"><img src="https://img.shields.io/github/stars/gordonhu608/MQT-LLaVA?label=MQT LLaVA&logo=github&style=flat-square" height="16"></a> 
- Efficient Time Series Processing for **Transformers and State-Space Models** through Token Merging <a href="https://arxiv.org/abs/2405.17951"><img src="https://img.shields.io/badge/arXiv-2405.17951-b31b1b?logo=arxiv" height="16"></a>
- **Matryoshka Multimodal Models** <a href="https://arxiv.org/abs/2405.17430"><img src="https://img.shields.io/badge/arXiv-2405.17430-b31b1b?logo=arxiv" height="16"></a>  <a href="https://matryoshka-mm.github.io/"><img src="https://img.shields.io/badge/Project-matryoshka-yellow" height="16"></a> 
- [NeurIPS 2024] Accelerating Transformers with **Spectrum-Preserving Token Merging** <a href="https://arxiv.org/abs/2405.16148"><img src="https://img.shields.io/badge/arXiv-2405.16148-b31b1b?logo=arxiv" height="16"></a>
- Efficient Point Transformer with **Dynamic Token Aggregating** for LiDAR Point Cloud Processing <a href="https://arxiv.org/abs/2405.15827"><img src="https://img.shields.io/badge/arXiv-2405.15827-b31b1b?logo=arxiv" height="16"></a>
- **Sparse-Tuning**: Adapting Vision Transformers with Efficient Fine-tuning and Inference <a href="https://arxiv.org/abs/2405.14700"><img src="https://img.shields.io/badge/arXiv-2405.14700-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/liuting20/Sparse-Tuning"><img src="https://img.shields.io/github/stars/liuting20/Sparse-Tuning?label=Sparse Tuning&logo=github&style=flat-square" height="16"></a> 
- [MIPR 2024] **Segformer++**: Efficient Token-Merging Strategies for High-Resolution Semantic Segmentation <a href="https://arxiv.org/abs/2405.14467"><img src="https://img.shields.io/badge/arXiv-2405.14467-b31b1b?logo=arxiv" height="16"></a>
- [AAAI 2025] **VTG-LLM**: Integrating Timestamp Knowledge into Video LLMs for Enhanced Video Temporal Grounding <a href="https://arxiv.org/abs/2405.13382"><img src="https://img.shields.io/badge/arXiv-2405.13382-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/gyxxyg/VTG-LLM"><img src="https://img.shields.io/github/stars/gyxxyg/VTG-LLM?label=VTG LLM&logo=github&style=flat-square" height="16"></a> 
- [CVPRW 2024] **Block Selective Reprogramming** for On-device Training of Vision Transformers <a href="https://arxiv.org/abs/2405.10951"><img src="https://img.shields.io/badge/arXiv-2405.10951-b31b1b?logo=arxiv" height="16"></a>
- [IJCAI 2024] **LeMeViT**: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation <a href="https://arxiv.org/abs/2405.09789"><img src="https://img.shields.io/badge/arXiv-2405.09789-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/ViTAE-Transformer/LeMeViT"><img src="https://img.shields.io/github/stars/ViTAE-Transformer/LeMeViT?label=LeMeViT&logo=github&style=flat-square" height="16"></a> 
- Boosting Multimodal Large Language Models with **Visual Tokens Withdrawal** for Rapid Inference <a href="https://arxiv.org/abs/2405.05803"><img src="https://img.shields.io/badge/arXiv-2405.05803-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] Attention-Driven Training-Free Efficiency Enhancement of **Diffusion Models** <a href="https://arxiv.org/abs/2405.05252"><img src="https://img.shields.io/badge/arXiv-2405.05252-b31b1b?logo=arxiv" height="16"></a>  <a href="https://atedm.github.io/"><img src="https://img.shields.io/badge/Project-atedm-yellow" height="16"></a> 
- [EMNLP 2024] **TinyChart**: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning <a href="https://arxiv.org/abs/2404.16635"><img src="https://img.shields.io/badge/arXiv-2404.16635-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart"><img src="https://img.shields.io/github/stars/X-PLUG/mPLUG-DocOwl?label=TinyChart&logo=github&style=flat-square" height="16"></a> 
- [ICLR 2024] **Hierarchical Context Merging**: Better Long Context Understanding for Pre-trained LLMs <a href="https://arxiv.org/abs/2404.10308"><img src="https://img.shields.io/badge/arXiv-2404.10308-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/alinlab/HOMER"><img src="https://img.shields.io/github/stars/alinlab/HOMER?label=HOMER&logo=github&style=flat-square" height="16"></a> 
- **Arena**: A Patch-of-Interest ViT Inference Acceleration System for Edge-Assisted Video Analytics <a href="https://arxiv.org/abs/2404.09245"><img src="https://img.shields.io/badge/arXiv-2404.09245-b31b1b?logo=arxiv" height="16"></a>
- **TextHawk**: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models <a href="https://arxiv.org/abs/2404.09204"><img src="https://img.shields.io/badge/arXiv-2404.09204-b31b1b?logo=arxiv" height="16"></a>
- **CATP**: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference <a href="https://arxiv.org/abs/2404.08567"><img src="https://img.shields.io/badge/arXiv-2404.08567-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] **HRVDA**: High-Resolution Visual Document Assistant <a href="https://arxiv.org/abs/2404.06918"><img src="https://img.shields.io/badge/arXiv-2404.06918-b31b1b?logo=arxiv" height="16"></a>
- **InternLM-XComposer2-4KHD**: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD <a href="https://arxiv.org/abs/2404.06512"><img src="https://img.shields.io/badge/arXiv-2404.06512-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/InternLM/InternLM-XComposer"><img src="https://img.shields.io/github/stars/InternLM/InternLM-XComposer?label=InternLM XComposer&logo=github&style=flat-square" height="16"></a> 
- [CVPR 2024] **MLP Can Be A Good Transformer Learner** <a href="https://arxiv.org/abs/2404.05657"><img src="https://img.shields.io/badge/arXiv-2404.05657-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/sihaoevery/lambda_vit"><img src="https://img.shields.io/github/stars/sihaoevery/lambda_vit?label=lambda_vit&logo=github&style=flat-square" height="16"></a> 
- [CVPR 2024] **Decoupling Static and Hierarchical Motion Perception** for Referring Video Segmentation <a href="https://arxiv.org/abs/2404.03645"><img src="https://img.shields.io/badge/arXiv-2404.03645-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/heshuting555/DsHmp"><img src="https://img.shields.io/github/stars/heshuting555/DsHmp?label=DsHmp&logo=github&style=flat-square" height="16"></a>
- Training LLMs over **Neurally Compressed Text**  <a href="https://arxiv.org/abs/2404.03626"><img src="https://img.shields.io/badge/arXiv-2404.03626-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **LongVLM**: Efficient Long Video Understanding via Large Language Models <a href="https://arxiv.org/abs/2404.03384"><img src="https://img.shields.io/badge/arXiv-2404.03384-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/ziplab/LongVLM"><img src="https://img.shields.io/github/stars/ziplab/LongVLM?label=LongVLM&logo=github&style=flat-square" height="16"></a>
- [CVPR 2024] **Learning to Rank Patches** for Unbiased Image Redundancy Reduction <a href="https://arxiv.org/abs/2404.00680"><img src="https://img.shields.io/badge/arXiv-2404.00680-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] A General and Efficient Training for Transformer via **Token Expansion** <a href="https://arxiv.org/abs/2404.00672"><img src="https://img.shields.io/badge/arXiv-2404.00672-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/Osilly/TokenExpansion"><img src="https://img.shields.io/github/stars/Osilly/TokenExpansion?label=TokenExpansion&logo=github&style=flat-square" height="16"></a>
- [CVPR 2024] **Dense Vision Transformer Compression** with Few Samples <a href="https://arxiv.org/abs/2403.18708"><img src="https://img.shields.io/badge/arXiv-2403.18708-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **Elysium**: Exploring Object-level Perception in Videos via MLLM <a href="https://arxiv.org/abs/2403.16558"><img src="https://img.shields.io/badge/arXiv-2403.16558-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/Hon-Wong/Elysium"><img src="https://img.shields.io/github/stars/Hon-Wong/Elysium?label=Elysium&logo=github&style=flat-square" height="16"></a>
- [ECCV 2024] **PaPr**: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference <a href="https://arxiv.org/abs/2403.16020"><img src="https://img.shields.io/badge/arXiv-2403.16020-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/tanvir-utexas/PaPr"><img src="https://img.shields.io/github/stars/tanvir-utexas/PaPr?label=PaPr&logo=github&style=flat-square" height="16"></a>
- **LLaVA-PruMerge**: Adaptive Token Reduction for Efficient Large Multimodal Models <a href="https://arxiv.org/abs/2403.15388"><img src="https://img.shields.io/badge/arXiv-2403.15388-b31b1b?logo=arxiv" height="16"></a>  <a href="https://llava-prumerge.github.io/"><img src="https://img.shields.io/badge/Project-llava_prumerge-yellow" height="16"></a>
- **FIT-RAG**: Black-Box RAG with Factual Information and Token Reduction  <a href="https://arxiv.org/abs/2403.14374"><img src="https://img.shields.io/badge/arXiv-2403.14374-b31b1b?logo=arxiv" height="16"></a>
- [FCCM 2024] Accelerating ViT Inference on FPGA through **Static and Dynamic Pruning** <a href="https://arxiv.org/abs/2403.14047"><img src="https://img.shields.io/badge/arXiv-2403.14047-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] **vid-TLDR**: Training Free Token Merging for Light-weight Video Transformer <a href="https://arxiv.org/abs/2403.13347"><img src="https://img.shields.io/badge/arXiv-2403.13347-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/mlvlab/vid-TLDR"><img src="https://img.shields.io/github/stars/mlvlab/vid-TLDR?label=vid_TLDR&logo=github&style=flat-square" height="16"></a>
- [ACM TIS 2024] An Analysis on **Matching Mechanisms and Token Pruning** for Late-interaction Models <a href="https://arxiv.org/abs/2403.13291"><img src="https://img.shields.io/badge/arXiv-2403.13291-b31b1b?logo=arxiv" height="16"></a>
- **HCPM**: Hierarchical Candidates Pruning for Efficient Detector-Free Matching <a href="https://arxiv.org/abs/2403.12543"><img src="https://img.shields.io/badge/arXiv-2403.12543-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2024] **Dynamic Tuning** Towards Parameter and Inference Efficiency for ViT Adaptation <a href="https://arxiv.org/abs/2403.11808"><img src="https://img.shields.io/badge/arXiv-2403.11808-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] **Multi-criteria Token Fusion** with One-step-ahead Attention for Efficient Vision Transformers  <a href="https://arxiv.org/abs/2403.10030"><img src="https://img.shields.io/badge/arXiv-2403.10030-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **PYRA**: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation <a href="https://arxiv.org/abs/2403.09192"><img src="https://img.shields.io/badge/arXiv-2403.09192-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/THU-MIG/PYRA"><img src="https://img.shields.io/github/stars/THU-MIG/PYRA?label=PYRA&logo=github&style=flat-square" height="16"></a>
- **Learnable Community-Aware Transformer** for Brain Connectome Analysis with Token Clustering <a href="https://arxiv.org/abs/2403.08203"><img src="https://img.shields.io/badge/arXiv-2403.08203-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024 Oral] **An Image is Worth 1/2 Tokens After Layer 2**: Plug-and-Play Acceleration for VLLM Inference <a href="https://arxiv.org/abs/2403.06764"><img src="https://img.shields.io/badge/arXiv-2403.06764-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/pkunlp-icler/FastV"><img src="https://img.shields.io/github/stars/pkunlp-icler/FastV?label=FastV&logo=github&style=flat-square" height="16"></a>
- [EMNLP 2024 Findings] **Unpacking Tokenization**: Evaluating Text Compression and its Correlation with Model Performance  <a href="https://arxiv.org/abs/2403.06265"><img src="https://img.shields.io/badge/arXiv-2403.06265-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **PixArt-Î£**: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation <a href="https://arxiv.org/abs/2403.04692"><img src="https://img.shields.io/badge/arXiv-2403.04692-b31b1b?logo=arxiv" height="16"></a>  <a href="https://pixart-alpha.github.io/PixArt-sigma-project/"><img src="https://img.shields.io/badge/Project-PixArt_sigma-yellow" height="16"></a> 
- [CVPR 2024] **MADTP**: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer <a href="https://arxiv.org/abs/2403.02991"><img src="https://img.shields.io/badge/arXiv-2403.02991-b31b1b?logo=arxiv" height="16"></a>
- **Motion Guided Token Compression** for Efficient Masked Video Modeling  <a href="https://arxiv.org/abs/2402.18577"><img src="https://img.shields.io/badge/arXiv-2402.18577-b31b1b?logo=arxiv" height="16"></a>
- [IJCAI 2024] **ToDo**: Token Downsampling for Efficient Generation of High-Resolution Images <a href="https://arxiv.org/abs/2402.13573"><img src="https://img.shields.io/badge/arXiv-2402.13573-b31b1b?logo=arxiv" height="16"></a>
- **MobileVLM V2**: Faster and Stronger Baseline for Vision Language Model <a href="https://arxiv.org/abs/2402.03766"><img src="https://img.shields.io/badge/arXiv-2402.03766-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/Meituan-AutoML/MobileVLM"><img src="https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM?label=MobileVLM&logo=github&style=flat-square" height="16"></a>
- Rethinking Optimization and Architecture for **Tiny Language Models**  <a href="https://arxiv.org/abs/2402.02791"><img src="https://img.shields.io/badge/arXiv-2402.02791-b31b1b?logo=arxiv" height="16"></a>
- **DeSparsify**: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers   <a href="https://arxiv.org/abs/2402.02554"><img src="https://img.shields.io/badge/arXiv-2402.02554-b31b1b?logo=arxiv" height="16"></a>
- **A Deep Hierarchical Feature Sparse Framework** for Occluded Person Re-Identification  <a href="https://arxiv.org/abs/2401.07469"><img src="https://img.shields.io/badge/arXiv-2401.07469-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **Object-Centric Diffusion** for Efficient Video Editing <a href="https://arxiv.org/abs/2401.05735"><img src="https://img.shields.io/badge/arXiv-2401.05735-b31b1b?logo=arxiv" height="16"></a>  <a href="http://qualcomm-ai-research.github.io/object-centric-diffusion"><img src="https://img.shields.io/badge/Project-object_centric_diffusion-yellow" height="16"></a> 
- [INFOCOM 2024] **OTAS**: An Elastic Transformer Serving System via Token Adaptation <a href="https://arxiv.org/abs/2401.05031"><img src="https://img.shields.io/badge/arXiv-2401.05031-b31b1b?logo=arxiv" height="16"></a>
- **HaltingVT**: Adaptive Token Halting Transformer for Efficient Video Recognition  <a href="https://arxiv.org/abs/2401.04975"><img src="https://img.shields.io/badge/arXiv-2401.04975-b31b1b?logo=arxiv" height="16"></a>
- [WACV 2024] **TPC-ViT**: Token Propagation Controller for Efficient Vision Transformer  <a href="https://arxiv.org/abs/2401.01470"><img src="https://img.shields.io/badge/arXiv-2401.01470-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **Removing Rows and Columns of Tokens in Vision Transformer** Enables Faster Dense Prediction without Retraining <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/09133.pdf"><img src="https://img.shields.io/badge/ECCV-2024-green" height="16"></a>  <a href="https://github.com/MilknoCandy/Token-Adapter"><img src="https://img.shields.io/github/stars/MilknoCandy/Token-Adapter?label=Token_Adapter&logo=github&style=flat-square" height="16"></a>
- [ACL 2024] Accelerating Transformers by **Sparsifying Information Flows** <a href="https://aclanthology.org/2024.acl-long.323.pdf"><img src="https://img.shields.io/badge/ACL-2024-green" height="16"></a>  <a href="https://github.com/yeachan-kr/sparseflow"><img src="https://img.shields.io/github/stars/yeachan-kr/sparseflow?label=sparseflow&logo=github&style=flat-square" height="16"></a>
- [ACL 2024 Findings] What Are You Token About? **Differentiable Perturbed Top-k Token Selection** for Scientific Document Summarization <a href="https://aclanthology.org/2024.findings-acl.561.pdf"><img src="https://img.shields.io/badge/ACL-2024-green" height="16"></a>  <a href="https://github.com/disi-unibo-nlp/sci-lay"><img src="https://img.shields.io/github/stars/disi-unibo-nlp/sci-lay?label=sci_lay&logo=github&style=flat-square" height="16"></a>
- [EMNLP 2024 Findings] **Vanessa**: Visual Connotation and Aesthetic Attributes Understanding Network for Multimodal Aspect-based Sentiment Analysis <a href="https://aclanthology.org/2024.findings-emnlp.671.pdf"><img src="https://img.shields.io/badge/EMNLP-2024-green" height="16"></a>
- [NeurIPS 2024] **MG-ViT**: A Multi-Granularity Method for Compact and Efficient Vision Transformers <a href="https://openreview.net/pdf?id=Bf6WFWNCUP"><img src="https://img.shields.io/badge/NeurIPS-2024-green" height="16"></a>
- **LVP**: Language-guided Visual Projector for Efficient Multimodal LLM <a href="https://openreview.net/pdf?id=PxBzxO02Ef"><img src="https://img.shields.io/badge/OpenReview-2024-orange" height="16"></a>
- [ECCV 2024] **IVTP**: Instruction-guided Visual Token Pruning for Large Vision-Language Models <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02577.pdf"><img src="https://img.shields.io/badge/ECCV-2024-green" height="16"></a>
- [AAAI 2024] **TMFormer**: Token Merging Transformer for Brain Tumor Segmentation with Missing Modalities <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28572"><img src="https://img.shields.io/badge/AAAI-2024-green" height="16"></a>
- **Connectivity-based Token Condensation** for Efficient Vision Transformer <a href="https://openreview.net/forum?id=8vGXHjuCiq"><img src="https://img.shields.io/badge/OpenReview-2024-orange" height="16"></a>
- [CVPRW 2024] Efficient Transformer Adaptation with **Soft Token Merging** <a href="https://openaccess.thecvf.com/content/CVPR2024W/ELVM/papers/Yuan_Efficient_Transformer_Adaptation_with_Soft_Token_Merging_CVPRW_2024_paper.pdf"><img src="https://img.shields.io/badge/CVPRW-2024-green" height="16"></a>
- **CAT Pruning**: Cluster-Aware Token Pruning For Text-to-Image Diffusion Models <a href="https://openreview.net/pdf/bf470617f541635cbde87fcc0ba3fdbddcef3db7.pdf"><img src="https://img.shields.io/badge/OpenReview-2024-orange" height="16"></a>  <a href="https://github.com/ada-cheng/CAT-Pruning"><img src="https://img.shields.io/github/stars/ada-cheng/CAT-Pruning?label=CAT_Pruning&logo=github&style=flat-square" height="16"></a>
- [ICLRW 2024] **Energy Minimizing-based Token Merging** for Accelerating Transformers <a href="https://openreview.net/forum?id=R7dCHc2Rp0"><img src="https://img.shields.io/badge/ICLRW-2024-green" height="16"></a>
- **RanMerFormer**: Randomized Vision Transformer with Token Merging for Brain Tumor Classification <a href="https://www.sciencedirect.com/science/article/pii/S0925231223013395?ref=pdf_download&fr=RR-2&rr=908e45ba48d2310f"><img src="https://img.shields.io/badge/Neurocomputing-2024-green" height="16"></a>
- [NeurIPSW 2024] **M2M-TAG**: Training-Free Many-to-Many Token Aggregation for Vision Transformer Acceleration <a href="https://openreview.net/pdf?id=LO3Mw8Jrk0"><img src="https://img.shields.io/badge/NeurIPSW-2024-green" height="16"></a>
- [ECCV 2024] Efficient Vision Transformers with **Partial Attention** <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11047.pdf"><img src="https://img.shields.io/badge/ECCV-2024-green" height="16"></a>

## 2023
- [CVPR 2023] **VidToMe**: Video Token Merging for Zero-Shot Video Editing <a href="https://arxiv.org/abs/2312.10656"><img src="https://img.shields.io/badge/arXiv-2312.10656-b31b1b?logo=arxiv" height="16"></a> <a href="https://vidtome-diffusion.github.io/"><img src="https://img.shields.io/badge/Project-vidtome-yellow" height="16"></a> 
- [ECCV 2024] **Agent attention**: On the integration of softmax and linear attention <a href="https://arxiv.org/abs/2312.08874"><img src="https://img.shields.io/badge/arXiv-2312.08874-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/LeapLabTHU/Agent-Attention"><img src="https://img.shields.io/github/stars/LeapLabTHU/Agent-Attention?label=Agent_Attention&logo=github&style=flat-square" height="16"></a>
- [CVPR 2024] **Honeybee**: Locality-enhanced Projector for Multimodal LLM <a href="https://arxiv.org/abs/2312.06742"><img src="https://img.shields.io/badge/arXiv-2312.06742-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/kakaobrain/honeybee"><img src="https://img.shields.io/github/stars/kakaobrain/honeybee?label=honeybee&logo=github&style=flat-square" height="16"></a>
- [AAAI 2024] **Agile-Quant**: Activation-Guided Quantization for Faster Inference of LLMs on the Edge <a href="https://arxiv.org/abs/2312.05693"><img src="https://img.shields.io/badge/arXiv-2312.05693-b31b1b?logo=arxiv" height="16"></a>
- [WACV 2024] **Token Fusion**: Bridging the Gap between Token Pruning and Token Merging <a href="https://arxiv.org/abs/2312.01026"><img src="https://img.shields.io/badge/arXiv-2312.01026-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **LLaMA-VID**: An Image is Worth 2 Tokens in Large Language Models <a href="https://arxiv.org/abs/2311.17043"><img src="https://img.shields.io/badge/arXiv-2311.17043-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/dvlab-research/LLaMA-VID"><img src="https://img.shields.io/github/stars/dvlab-research/LLaMA-VID?label=LLaMA_VID&logo=github&style=flat-square" height="16"></a>
- [CVPR 2024] **Align before Adapt**: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition <a href="https://arxiv.org/abs/2311.15619"><img src="https://img.shields.io/badge/arXiv-2311.15619-b31b1b?logo=arxiv" height="16"></a>
- [WACV 2025] **TORE**: Token Recycling in Vision Transformers for Efficient Active Visual Exploration <a href="https://arxiv.org/abs/2311.15335"><img src="https://img.shields.io/badge/arXiv-2311.15335-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] **Hourglass Tokenizer** for Efficient Transformer-Based 3D Human Pose Estimation <a href="https://arxiv.org/abs/2311.12028"><img src="https://img.shields.io/badge/arXiv-2311.12028-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/NationalGAILab/HoT"><img src="https://img.shields.io/github/stars/NationalGAILab/HoT?label=HoT&logo=github&style=flat-square" height="16"></a>
- [ICASSP 2023] **SparseSpikformer**: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer <a href="https://arxiv.org/abs/2311.08806"><img src="https://img.shields.io/badge/arXiv-2311.08806-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024 Highlight] **Chat-UniVi**: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding <a href="https://arxiv.org/abs/2311.08046"><img src="https://img.shields.io/badge/arXiv-2311.08046-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/PKU-YuanGroup/Chat-UniVi"><img src="https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi?label=Chat_UniVi&logo=github&style=flat-square" height="16"></a>
- [WACV 2024 Oral] **GTP-ViT**: Efficient Vision Transformers via Graph-based Token Propagation <a href="https://arxiv.org/abs/2311.03035"><img src="https://img.shields.io/badge/arXiv-2311.03035-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/Ackesnal/GTP-ViT"><img src="https://img.shields.io/github/stars/Ackesnal/GTP-ViT?label=GTP_ViT&logo=github&style=flat-square" height="16"></a>
- [NeurIPS 2023] **AiluRus**: A Scalable ViT Framework for Dense Prediction <a href="https://arxiv.org/abs/2311.01197"><img src="https://img.shields.io/badge/arXiv-2311.01197-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/caddyless/ailurus"><img src="https://img.shields.io/github/stars/caddyless/ailurus?label=ailurus&logo=github&style=flat-square" height="16"></a>
- [EMNLP 2023] **TESTA**: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding <a href="https://arxiv.org/abs/2310.19060"><img src="https://img.shields.io/badge/arXiv-2310.19060-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/RenShuhuai-Andy/TESTA"><img src="https://img.shields.io/github/stars/RenShuhuai-Andy/TESTA?label=TESTA&logo=github&style=flat-square" height="16"></a>
- Bridging The Gaps Between Token Pruning and Full Pre-training via **Masked Fine-tuning** <a href="https://arxiv.org/abs/2310.17177"><img src="https://img.shields.io/badge/arXiv-2310.17177-b31b1b?logo=arxiv" height="16"></a>
- [EMNLP 2023 Findings] **Variator**: Accelerating Pre-trained Models with Plug-and-Play Compression Modules <a href="https://arxiv.org/abs/2310.15724"><img src="https://img.shields.io/badge/arXiv-2310.15724-b31b1b?logo=arxiv" height="16"></a>
- [EMNLP 2023 Findings] **TCRA-LLM**: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction  <a href="https://arxiv.org/abs/2310.15556"><img src="https://img.shields.io/badge/arXiv-2310.15556-b31b1b?logo=arxiv" height="16"></a>
- **NSM4D**: Neural Scene Model Based Online 4D Point Cloud Sequence Understanding <a href="https://arxiv.org/abs/2310.08326"><img src="https://img.shields.io/badge/arXiv-2310.08326-b31b1b?logo=arxiv" height="16"></a>
- **SparseCoder**: Advancing Source Code Analysis with Sparse Attention and Learned Token Pruning  <a href="https://arxiv.org/abs/2310.07109"><img src="https://img.shields.io/badge/arXiv-2310.07109-b31b1b?logo=arxiv" height="16"></a>
- [Applied Informatics 2023] **No Token Left Behind**: Efficient Vision Transformer via Dynamic Token Idling <a href="https://arxiv.org/abs/2310.05654"><img src="https://img.shields.io/badge/arXiv-2310.05654-b31b1b?logo=arxiv" height="16"></a>
- [ACL 2024] Expedited Training of **Visual Conditioned Language Generation** via Redundancy Reduction <a href="https://arxiv.org/abs/2310.03291"><img src="https://img.shields.io/badge/arXiv-2310.03291-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/yiren-jian/EVLGen"><img src="https://img.shields.io/github/stars/yiren-jian/EVLGen?label=EVLGen&logo=github&style=flat-square" height="16"></a>
- **PPT**: Token Pruning and Pooling for Efficient Vision Transformers <a href="https://arxiv.org/abs/2310.01812"><img src="https://img.shields.io/badge/arXiv-2310.01812-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/xjwu1024/PPT"><img src="https://img.shields.io/github/stars/xjwu1024/PPT?label=PPT&logo=github&style=flat-square" height="16"></a>
- **ELIP**: Efficient Language-Image Pre-training with Fewer Vision Tokens <a href="https://arxiv.org/abs/2309.16738"><img src="https://img.shields.io/badge/arXiv-2309.16738-b31b1b?logo=arxiv" height="16"></a>
- **CAIT**: Triple-Win Compression towards High Accuracy, Fast Inference, and Favorable Transferability For ViTs <a href="https://arxiv.org/abs/2309.15755"><img src="https://img.shields.io/badge/arXiv-2309.15755-b31b1b?logo=arxiv" height="16"></a>
- [ICLR 2024] Unified Language-Vision Pretraining in LLM with **Dynamic Discrete Visual Tokenization** <a href="https://arxiv.org/abs/2309.04669"><img src="https://img.shields.io/badge/arXiv-2309.04669-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/jy0205/LaVIT"><img src="https://img.shields.io/github/stars/jy0205/LaVIT?label=LaVIT&logo=github&style=flat-square" height="16"></a>
- [ICCV 2023] Attention Where It Matters: Rethinking Visual Document Understanding with **Selective Region Concentration**  <a href="https://arxiv.org/abs/2309.01131"><img src="https://img.shields.io/badge/arXiv-2309.01131-b31b1b?logo=arxiv" height="16"></a>
- [ICCV 2023] **SG-Former**: Self-guided Transformer with Evolving Token Reallocation <a href="https://arxiv.org/abs/2308.12216"><img src="https://img.shields.io/badge/arXiv-2308.12216-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/OliverRensu/SG-Former"><img src="https://img.shields.io/github/stars/OliverRensu/SG-Former?label=SG_Former&logo=github&style=flat-square" height="16"></a>
- [ICCV 2023] **Isomer**: Isomerous Transformer for Zero-shot Video Object Segmentation <a href="https://arxiv.org/abs/2308.06693"><img src="https://img.shields.io/badge/arXiv-2308.06693-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/DLUT-yyc/Isomer"><img src="https://img.shields.io/github/stars/DLUT-yyc/Isomer?label=Isomer&logo=github&style=flat-square" height="16"></a>
- [ICCVW 2023] Which Tokens to Use? Investigating Token Reduction in **Vision Transformers** <a href="https://arxiv.org/abs/2308.04657"><img src="https://img.shields.io/badge/arXiv-2308.04657-b31b1b?logo=arxiv" height="16"></a>  <a href="=https://vap.aau.dk/tokens"><img src="https://img.shields.io/badge/Project-tokens-yellow" height="16"></a> 
- [ICCV 2023] Prune Spatio-temporal Tokens by **Semantic-aware Temporal Accumulation** <a href="https://arxiv.org/abs/2308.04549"><img src="https://img.shields.io/badge/arXiv-2308.04549-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/Mark12Ding/STA"><img src="https://img.shields.io/github/stars/Mark12Ding/STA?label=STA&logo=github&style=flat-square" height="16"></a>
- **DiT**: Efficient Vision Transformers with Dynamic Token Routing <a href="https://arxiv.org/abs/2308.03409"><img src="https://img.shields.io/badge/arXiv-2308.03409-b31b1b?logo=arxiv" height="16"></a>
- [WACV 2023] **Dynamic Token-Pass Transformers** for Semantic Segmentation <a href="https://arxiv.org/abs/2308.01944"><img src="https://img.shields.io/badge/arXiv-2308.01944-b31b1b?logo=arxiv" height="16"></a>
- [ICCV 2023] **Dynamic Token Pruning** in Plain Vision Transformers for Semantic Segmentation <a href="https://arxiv.org/abs/2308.01045"><img src="https://img.shields.io/badge/arXiv-2308.01045-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2024] **MovieChat**: From Dense Token to Sparse Memory for Long Video Understanding <a href="https://arxiv.org/abs/2307.16449"><img src="https://img.shields.io/badge/arXiv-2307.16449-b31b1b?logo=arxiv" height="16"></a>  <a href="=https://rese1f.github.io/MovieChat/"><img src="https://img.shields.io/badge/Project-MovieChat-yellow" height="16"></a> 
- [TMLR 2023] **Learned Thresholds Token Merging and Pruning** for Vision Transformers <a href="https://arxiv.org/abs/2307.10780"><img src="https://img.shields.io/badge/arXiv-2307.10780-b31b1b?logo=arxiv" height="16"></a>
- [ICCVW 2023] **MSViT**: Dynamic Mixed-Scale Tokenization for Vision Transformers <a href="https://arxiv.org/abs/2307.02321"><img src="https://img.shields.io/badge/arXiv-2307.02321-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/Qualcomm-AI-research/batchshaping"><img src="https://img.shields.io/github/stars/Qualcomm-AI-research/batchshaping?label=batchshaping&logo=github&style=flat-square" height="16"></a>
- [EMNLP 2024] **LOOK-M**: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference <a href="https://arxiv.org/abs/2406.18139"><img src="https://img.shields.io/badge/arXiv-2406.18139-b31b1b?logo=arxiv" height="16"></a> 
- [Interspeech 2023] Accelerating Transducers through **Adjacent Token Merging** <a href="https://arxiv.org/abs/2306.16009"><img src="https://img.shields.io/badge/arXiv-2306.16009-b31b1b?logo=arxiv" height="16"></a>
- [KDD 2023] **Constraint-aware and Ranking-distilled Token Pruning** for Efficient Transformer Inference <a href="https://arxiv.org/abs/2306.14393"><img src="https://img.shields.io/badge/arXiv-2306.14393-b31b1b?logo=arxiv" height="16"></a>
- Vision Transformer with **Attention Map Hallucination and FFN Compaction**   <a href="https://arxiv.org/abs/2306.10875"><img src="https://img.shields.io/badge/arXiv-2306.10875-b31b1b?logo=arxiv" height="16"></a>
- [WACV 2023] Revisiting Token Pruning for **Object Detection and Instance Segmentation** <a href="https://arxiv.org/abs/2306.07050"><img src="https://img.shields.io/badge/arXiv-2306.07050-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/uzh-rpg/svit"><img src="https://img.shields.io/github/stars/uzh-rpg/svit?label=svit&logo=github&style=flat-square" height="16"></a>
- **Multi-Scale And Token Mergence**: Make Your ViT More Efficient <a href="https://arxiv.org/abs/2306.04897"><img src="https://img.shields.io/badge/arXiv-2306.04897-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2023] **Content-aware Token Sharing** for Efficient Semantic Segmentation with Vision Transformers  <a href="https://arxiv.org/abs/2306.02095"><img src="https://img.shields.io/badge/arXiv-2306.02095-b31b1b?logo=arxiv" height="16"></a> <a href="=https://tue-mps.github.io/CTS/"><img src="https://img.shields.io/badge/Project-CTS-yellow" height="16"></a> 
- [ICCV 2023] **DiffRate**: Differentiable Compression Rate for Efficient Vision Transformers <a href="https://arxiv.org/abs/2305.17997"><img src="https://img.shields.io/badge/arXiv-2305.17997-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/OpenGVLab/DiffRate"><img src="https://img.shields.io/github/stars/OpenGVLab/DiffRate?label=DiffRate&logo=github&style=flat-square" height="16"></a>
- [ACL 2023] **PuMer**: Pruning and Merging Tokens for Efficient Vision Language Models <a href="https://arxiv.org/abs/2305.17530"><img src="https://img.shields.io/badge/arXiv-2305.17530-b31b1b?logo=arxiv" height="16"></a>
- [ICML 2024] **CrossGET**: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers <a href="https://arxiv.org/abs/2305.17455"><img src="https://img.shields.io/badge/arXiv-2305.17455-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/sdc17/CrossGET"><img src="https://img.shields.io/github/stars/sdc17/CrossGET?label=CrossGET&logo=github&style=flat-square" height="16"></a>
- [CVPR 2024] **Zero-TPrune**: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers <a href="https://arxiv.org/abs/2305.17328"><img src="https://img.shields.io/badge/arXiv-2305.17328-b31b1b?logo=arxiv" height="16"></a>  <a href="=https://jha-lab.github.io/zerotprunet"><img src="https://img.shields.io/badge/Project-zerotprunet-yellow" height="16"></a>
- **Multi-scale Efficient Graph-Transformer** for Whole Slide Image Classification  <a href="https://arxiv.org/abs/2305.15773"><img src="https://img.shields.io/badge/arXiv-2305.15773-b31b1b?logo=arxiv" height="16"></a>
- [LREC 2024] **SmartTrim**: Adaptive Tokens and Attention Pruning for Efficient Vision-Language Models <a href="https://arxiv.org/abs/2305.15033"><img src="https://img.shields.io/badge/arXiv-2305.15033-b31b1b?logo=arxiv" height="16"></a> 
- **Infor-Coef**: Information Bottleneck-based Dynamic Token Downsampling for Compact and Efficient Language Model <a href="https://arxiv.org/abs/2305.12458"><img src="https://img.shields.io/badge/arXiv-2305.12458-b31b1b?logo=arxiv" height="16"></a>
- [IJCAI 2023] **Fast-StrucTexT**: An Efficient Hourglass Transformer with Modality-guided Dynamic Token Merge for Document Understanding <a href="https://arxiv.org/abs/2305.11392"><img src="https://img.shields.io/badge/arXiv-2305.11392-b31b1b?logo=arxiv" height="16"></a>
- [IJCAI 2023] **TG-VQA**: Ternary Game of Video Question Answering <a href="https://arxiv.org/abs/2305.10049"><img src="https://img.shields.io/badge/arXiv-2305.10049-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2023] **Joint Token Pruning and Squeezing** Towards More Aggressive Compression of Vision Transformers <a href="https://arxiv.org/abs/2304.10716"><img src="https://img.shields.io/badge/arXiv-2304.10716-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/megvii-research/TPS-CVPR2023"><img src="https://img.shields.io/github/stars/megvii-research/TPS-CVPR2023?label=TPS&logo=github&style=flat-square" height="16"></a>
- [CVPR 2023] **SViTT**: Temporal Learning of Sparse Video-Text Transformers <a href="https://arxiv.org/abs/2304.08809"><img src="https://img.shields.io/badge/arXiv-2304.08809-b31b1b?logo=arxiv" height="16"></a>  <a href="=http://svcl.ucsd.edu/projects/svitt"><img src="https://img.shields.io/badge/Project-svitt-yellow" height="16"></a> 
- [ICCV 2023] Efficient Video Action Detection with **Token Dropout and Context Refinement** <a href="https://arxiv.org/abs/2304.08451"><img src="https://img.shields.io/badge/arXiv-2304.08451-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/MCG-NJU/EVAD"><img src="https://img.shields.io/github/stars/MCG-NJU/EVAD?label=EVAD&logo=github&style=flat-square" height="16"></a>
- **Distilling Token-Pruned Pose Transformer** for 2D Human Pose Estimation <a href="https://arxiv.org/abs/2304.05548"><img src="https://img.shields.io/badge/arXiv-2304.05548-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2023] **Conditional Adapters**: Parameter-efficient Transfer Learning with Fast Inference <a href="https://arxiv.org/abs/2304.04947"><img src="https://img.shields.io/badge/arXiv-2304.04947-b31b1b?logo=arxiv" height="16"></a>
- [TMM 2023] **Attention Map Guided Transformer Pruning** for Edge Device <a href="https://arxiv.org/abs/2304.01452"><img src="https://img.shields.io/badge/arXiv-2304.01452-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2023] **SparseViT**: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformers <a href="https://arxiv.org/abs/2303.17605"><img src="https://img.shields.io/badge/arXiv-2303.17605-b31b1b?logo=arxiv" height="16"></a>  <a href="https://sparsevit.mit.edu/"><img src="https://img.shields.io/badge/Project-sparsevit-yellow" height="16"></a> 
- [CVPRW 2023] Token Merging for **Fast Stable Diffusion** <a href="https://arxiv.org/abs/2303.17604"><img src="https://img.shields.io/badge/arXiv-2303.17604-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/dbolya/tomesd"><img src="https://img.shields.io/github/stars/dbolya/tomesd?label=tomesd&logo=github&style=flat-square" height="16"></a>
- [CVPR 2023] **Selective Structured State-Spaces** for Long-Form Video Understanding  <a href="https://arxiv.org/abs/2303.14526"><img src="https://img.shields.io/badge/arXiv-2303.14526-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2023 Highlight] Video-Text as Game Players: **Hierarchical Banzhaf Interaction** for Cross-Modal Representation Learning <a href="https://arxiv.org/abs/2303.14369"><img src="https://img.shields.io/badge/arXiv-2303.14369-b31b1b?logo=arxiv" height="16"></a>  <a href="https://jpthu17.github.io/HBI/"><img src="https://img.shields.io/badge/Project-HBI-yellow" height="16"></a> 
- [CVPR 2023] Making Vision Transformers Efficient from A **Token Sparsification View** <a href="https://arxiv.org/abs/2303.08685"><img src="https://img.shields.io/badge/arXiv-2303.08685-b31b1b?logo=arxiv" height="16"></a>  <a href="http://github.com/changsn/STViT-R"><img src="https://img.shields.io/github/stars/changsn/STViT-R?label=STViT-R&logo=github&style=flat-square" height="16"></a>
- [IPMI 2023] Token Sparsification for **Faster Medical Image Segmentation**  <a href="https://arxiv.org/abs/2303.06522"><img src="https://img.shields.io/badge/arXiv-2303.06522-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/cvlab-stonybrook/TokenSparse-for-MedSeg"><img src="https://img.shields.io/github/stars/cvlab-stonybrook/TokenSparse-for-MedSeg?label=TokenSparse&logo=github&style=flat-square" height="16"></a>
- [ICMLW 2024] Training-Free Visual Token Compression via **Delayed Spatial Merging** <a href="https://arxiv.org/abs/2303.02331"><img src="https://img.shields.io/badge/arXiv-2303.02331-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2024] **The Role of Masking** for Efficient Supervised Knowledge Distillation of Vision Transformers <a href="https://arxiv.org/abs/2302.10494"><img src="https://img.shields.io/badge/arXiv-2302.10494-b31b1b?logo=arxiv" height="16"></a>
- [ICLR 2023] A Theoretical Understanding of **Shallow Vision Transformers**: Learning, Generalization, and Sample Complexity  <a href="https://arxiv.org/abs/2302.06015"><img src="https://img.shields.io/badge/arXiv-2302.06015-b31b1b?logo=arxiv" height="16"></a>
- [ICML 2023] **BLIP-2**: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models <a href="https://arxiv.org/abs/2301.12597"><img src="https://img.shields.io/badge/arXiv-2301.12597-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/salesforce/LAVIS/tree/main/projects/blip2"><img src="https://img.shields.io/github/stars/salesforce/LAVIS?label=blip2&logo=github&style=flat-square" height="16"></a>
- **Image Compression Is an Effective Objective** for Visual Representation Learning <a href="https://openreview.net/forum?id=ickxszYnwH"><img src="https://img.shields.io/badge/OpenReview-2023-orange" height="16"></a>
- [EMNLP 2023 Findings] **Length-Adaptive Distillation**: Customizing Small Language Model for Dynamic Token Pruning <a href="https://aclanthology.org/2023.findings-emnlp.294.pdf"><img src="https://img.shields.io/badge/EMNLP-2023-green" height="16"></a>
- [EMNLP 2023] **Leap-of-Thought**: Accelerating Transformers via Dynamic Token Routing <a href="https://aclanthology.org/2023.emnlp-main.976.pdf"><img src="https://img.shields.io/badge/EMNLP-2023-green" height="16"></a>  <a href="https://github.com/yeachan-kr/lot"><img src="https://img.shields.io/github/stars/yeachan-kr/lot?label=lot&logo=github&style=flat-square" height="16"></a>
- [ICCV 2023] Building Vision Transformers with **Hierarchy Aware Feature Aggregation** <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Building_Vision_Transformers_with_Hierarchy_Aware_Feature_Aggregation_ICCV_2023_paper.pdf"><img src="https://img.shields.io/badge/ICCV-2023-green" height="16"></a>
- [CVPR 2023] Dynamic Inference with **Grounding Based Vision and Language Models** <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Uzkent_Dynamic_Inference_With_Grounding_Based_Vision_and_Language_Models_CVPR_2023_paper.pdf"><img src="https://img.shields.io/badge/CVPR-2023-green" height="16"></a>
- [ICLR 2023] **Sparse Token Transformer** With Attention Back Tracking <a href="https://openreview.net/forum?id=VV0hSE8AxCw"><img src="https://img.shields.io/badge/ICLR-2023-green" height="16"></a>
- [ICLR 2023] **Progressively Compressed Auto-Encoder** for Self-supervised Representation Learning <a href="https://openreview.net/pdf?id=8T4qmZbTkW7"><img src="https://img.shields.io/badge/ICLR-2023-green" height="16"></a>  <a href="https://github.com/caddyless/PCAE"><img src="https://img.shields.io/github/stars/caddyless/PCAE?label=PCAE&logo=github&style=flat-square" height="16"></a>

## 2022
- **SMAUG**: Sparse Masked Autoencoder for Efficient Video-Language Pre-training  <a href="https://arxiv.org/abs/2211.11446"><img src="https://img.shields.io/badge/arXiv-2211.11446-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2023] Beyond Attentive Tokens: **Incorporating Token Importance and Diversity** for Efficient Vision Transformers <a href="https://arxiv.org/abs/2211.11315"><img src="https://img.shields.io/badge/arXiv-2211.11315-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/BWLONG/BeyondAttentiveTokens"><img src="https://img.shields.io/github/stars/BWLONG/BeyondAttentiveTokens?label=BeyondAttentiveTokens&logo=github&style=flat-square" height="16"></a>
- [ICCV 2023] **TORE**: Token Reduction for Efficient Human Mesh Recovery with Transformer <a href="https://arxiv.org/abs/2211.10705"><img src="https://img.shields.io/badge/arXiv-2211.10705-b31b1b?logo=arxiv" height="16"></a>  <a href="https://frank-zy-dou.github.io/projects/Tore/index.html"><img src="https://img.shields.io/badge/Project-Tore-yellow" height="16"></a>
- [HPCA 2023] **HeatViT**: Hardware-Efficient Adaptive Token Pruning for Vision Transformers  <a href="https://arxiv.org/abs/2211.08110"><img src="https://img.shields.io/badge/arXiv-2211.08110-b31b1b?logo=arxiv" height="16"></a>
- [ICCV 2023] **Fcaformer**: Forward Cross Attention in Hybrid Vision Transformer <a href="https://arxiv.org/abs/2211.07198"><img src="https://img.shields.io/badge/arXiv-2211.07198-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/hkzhang91/FcaFormer"><img src="https://img.shields.io/github/stars/hkzhang91/FcaFormer?label=FcaFormer&logo=github&style=flat-square" height="16"></a>
- [ICASSP 2023] **ProContEXT**: Exploring Progressive Context Transformer for Tracking  <a href="https://arxiv.org/abs/2210.15511"><img src="https://img.shields.io/badge/arXiv-2210.15511-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/zhiqic/ProContEXT"><img src="https://img.shields.io/github/stars/zhiqic/ProContEXT?label=ProContEXT&logo=github&style=flat-square" height="16"></a>
- [ICLR 2023 Oral] **Token Merging**: Your ViT But Faster <a href="https://arxiv.org/abs/2210.09461"><img src="https://img.shields.io/badge/arXiv-2210.09461-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/facebookresearch/ToMe"><img src="https://img.shields.io/github/stars/facebookresearch/ToMe?label=ToMe&logo=github&style=flat-square" height="16"></a>
- **SaiT**: Sparse Vision Transformers through Adaptive Token Pruning <a href="https://arxiv.org/abs/2210.05832"><img src="https://img.shields.io/badge/arXiv-2210.05832-b31b1b?logo=arxiv" height="16"></a>
- [IJCAI 2023] **Adaptive Sparse ViT**: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention <a href="https://arxiv.org/abs/2209.13802"><img src="https://img.shields.io/badge/arXiv-2209.13802-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/Cydia2018/AS-ViT"><img src="https://img.shields.io/github/stars/Cydia2018/AS-ViT?label=AS_ViT&logo=github&style=flat-square" height="16"></a>
- [ECCV 2022] **PPT**: token-Pruned Pose Transformer for monocular and multi-view human pose estimation  <a href="https://arxiv.org/abs/2209.08194"><img src="https://img.shields.io/badge/arXiv-2209.08194-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/HowieMa/PPT"><img src="https://img.shields.io/github/stars/HowieMa/PPT?label=PPT&logo=github&style=flat-square" height="16"></a>
- [TPAMI 2022] **Dynamic Spatial Sparsification** for Efficient Vision Transformers and Convolutional Neural Networks <a href="https://arxiv.org/abs/2207.01580"><img src="https://img.shields.io/badge/arXiv-2207.01580-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/raoyongming/DynamicViT"><img src="https://img.shields.io/github/stars/raoyongming/DynamicViT?label=DynamicViT&logo=github&style=flat-square" height="16"></a>
- [ACL 2022] **Transkimmer**: Transformer Learns to Layer-wise Skim <a href="https://arxiv.org/abs/2205.07324"><img src="https://img.shields.io/badge/arXiv-2205.07324-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2022 Oral] Not All Tokens Are Equal: Human-centric Visual Analysis via **Token Clustering Transformer** <a href="https://arxiv.org/abs/2204.08680"><img src="https://img.shields.io/badge/arXiv-2204.08680-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/zengwang430521/TCFormer"><img src="https://img.shields.io/github/stars/zengwang430521/TCFormer?label=TCFormer&logo=github&style=flat-square" height="16"></a>
- **ITTR**: Unpaired Image-to-Image Translation with Transformers <a href="https://arxiv.org/abs/2203.16015"><img src="https://img.shields.io/badge/arXiv-2203.16015-b31b1b?logo=arxiv" height="16"></a>
- [AAAI 2023] **CF-ViT**: A General Coarse-to-Fine Method for Vision Transformer <a href="https://arxiv.org/abs/2203.03821"><img src="https://img.shields.io/badge/arXiv-2203.03821-b31b1b?logo=arxiv" height="16"></a>
- [Neural Networks 2022] **Multi-Tailed Vision Transformer** for Efficient Inference <a href="https://arxiv.org/abs/2203.01587"><img src="https://img.shields.io/badge/arXiv-2203.01587-b31b1b?logo=arxiv" height="16"></a>
- [ICLR 2022 Spotlight] Not All Patches are What You Need: Expediting Vision Transformers via **Token Reorganizations** <a href="https://arxiv.org/abs/2202.07800"><img src="https://img.shields.io/badge/arXiv-2202.07800-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/youweiliang/evit"><img src="https://img.shields.io/github/stars/youweiliang/evit?label=evit&logo=github&style=flat-square" height="16"></a>
- [CVPR 2022] **Vision Transformer Slimming**: Multi-Dimension Searching in Continuous Optimization Space <a href="https://arxiv.org/abs/2201.00814"><img src="https://img.shields.io/badge/arXiv-2201.00814-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/Arnav0400/ViT-Slim"><img src="https://img.shields.io/github/stars/Arnav0400/ViT-Slim?label=ViT_Slim&logo=github&style=flat-square" height="16"></a>
- [COLING 2022] **Token and Head Adaptive Transformers** for Efficient Natural Language Processing <a href="https://aclanthology.org/2022.coling-1.404.pdf"><img src="https://img.shields.io/badge/COLING-2022-green" height="16"></a>
- **HFSP**: A Hardware-friendly Soft Pruning Framework for Vision Transformers <a href="https://openreview.net/forum?id=dhLChxJwgMR"><img src="https://img.shields.io/badge/OpenReview-2022-orange" height="16"></a>

## 2021
- [ECCV 2022] **SPViT**: Enabling Faster Vision Transformers via Latency-aware Soft Token Pruning <a href="https://arxiv.org/abs/2112.13890"><img src="https://img.shields.io/badge/arXiv-2112.13890-b31b1b?logo=arxiv" height="16"></a>
- [CVPR 2022 Oral] **AdaViT**: Adaptive Tokens for Efficient Vision Transformer <a href="https://arxiv.org/abs/2112.07658"><img src="https://img.shields.io/badge/arXiv-2112.07658-b31b1b?logo=arxiv" height="16"></a> <a href="https://a-vit.github.io/"><img src="https://img.shields.io/badge/Project-a_vit-yellow" height="16"></a>
- A Study on **Token Pruning for ColBERT**  <a href="https://arxiv.org/abs/2112.06540"><img src="https://img.shields.io/badge/arXiv-2112.06540-b31b1b?logo=arxiv" height="16"></a>
- [ECCV 2022 Oral] **Adaptive Token Sampling** For Efficient Vision Transformers <a href="https://arxiv.org/abs/2111.15667"><img src="https://img.shields.io/badge/arXiv-2111.15667-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/adaptivetokensampling/ATS"><img src="https://img.shields.io/github/stars/adaptivetokensampling/ATS?label=ATS&logo=github&style=flat-square" height="16"></a>
- [ECCV 2022] **Self-slimmed Vision Transformer** <a href="https://arxiv.org/abs/2111.12624"><img src="https://img.shields.io/badge/arXiv-2111.12624-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/Sense-X/SiT"><img src="https://img.shields.io/github/stars/Sense-X/SiT?label=SiT&logo=github&style=flat-square" height="16"></a>
- [ECCV 2022] Efficient Video Transformers with **Spatial-Temporal Token Selection** <a href="https://arxiv.org/abs/2111.11591"><img src="https://img.shields.io/badge/arXiv-2111.11591-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/wangjk666/STTS"><img src="https://img.shields.io/github/stars/wangjk666/STTS?label=STTS&logo=github&style=flat-square" height="16"></a>
- **Magic Pyramid**: Accelerating Inference with Early Exiting and Token Pruning <a href="https://arxiv.org/abs/2111.00230"><img src="https://img.shields.io/badge/arXiv-2111.00230-b31b1b?logo=arxiv" height="16"></a>
- [WACV 2023] **Token Pooling** in Vision Transformers <a href="https://arxiv.org/abs/2110.03860"><img src="https://img.shields.io/badge/arXiv-2110.03860-b31b1b?logo=arxiv" height="16"></a>
- [AAAI 2022] **Evo-ViT**: Slow-Fast Token Evolution for Dynamic Vision Transformer <a href="https://arxiv.org/abs/2108.01390"><img src="https://img.shields.io/badge/arXiv-2108.01390-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/YifanXu74/Evo-ViT"><img src="https://img.shields.io/github/stars/YifanXu74/Evo-ViT?label=Evo_ViT&logo=github&style=flat-square" height="16"></a>
- [KDD 2022] **Learned Token Pruning** for Transformers <a href="https://arxiv.org/abs/2107.00910"><img src="https://img.shields.io/badge/arXiv-2107.00910-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2021] **IA-RED2**: Interpretability-Aware Redundancy Reduction for Vision Transformers <a href="https://arxiv.org/abs/2106.12620"><img src="https://img.shields.io/badge/arXiv-2106.12620-b31b1b?logo=arxiv" height="16"></a> <a href="http://people.csail.mit.edu/bpan/ia-red/"><img src="https://img.shields.io/badge/Project-ia_red-yellow" height="16"></a> 
- [NeurIPS 2021] **TokenLearner**: What Can 8 Learned Tokens Do for Images and Videos? <a href="https://arxiv.org/abs/2106.11297"><img src="https://img.shields.io/badge/arXiv-2106.11297-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner"><img src="https://img.shields.io/github/stars/google-research/scenic?label=token_learner&logo=github&style=flat-square" height="16"></a>
- [NeurIPS 2021] **Chasing Sparsity in Vision Transformers**: An End-to-End Exploration <a href="https://arxiv.org/abs/2106.04533"><img src="https://img.shields.io/badge/arXiv-2106.04533-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/VITA-Group/SViTE"><img src="https://img.shields.io/github/stars/VITA-Group/SViTE?label=SViTE&logo=github&style=flat-square" height="16"></a>
- **Shuffle Transformer**: Rethinking Spatial Shuffle for Vision Transformer <a href="https://arxiv.org/abs/2106.03650"><img src="https://img.shields.io/badge/arXiv-2106.03650-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/mulinmeng/Shuffle-Transformer"><img src="https://img.shields.io/github/stars/mulinmeng/Shuffle-Transformer?label=Shuffle_Transformer&logo=github&style=flat-square" height="16"></a>
- [CVPR 2022] **Patch Slimming** for Efficient Vision Transformers <a href="https://arxiv.org/abs/2106.02852"><img src="https://img.shields.io/badge/arXiv-2106.02852-b31b1b?logo=arxiv" height="16"></a>
- [NeurIPS 2021] **DynamicViT**: Efficient Vision Transformers with Dynamic Token Sparsification <a href="https://arxiv.org/abs/2106.02034"><img src="https://img.shields.io/badge/arXiv-2106.02034-b31b1b?logo=arxiv" height="16"></a>  <a href="https://dynamicvit.ivg-research.xyz/"><img src="https://img.shields.io/badge/Project-dynamicvit-yellow" height="16"></a> 
- [AAAI 2022] **Less is More**: Pay Less Attention in Vision Transformers <a href="https://arxiv.org/abs/2105.14217"><img src="https://img.shields.io/badge/arXiv-2105.14217-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/zhuang-group/LIT"><img src="https://img.shields.io/github/stars/zhuang-group/LIT?label=LIT&logo=github&style=flat-square" height="16"></a>
- [NAACL 2021] **TR-BERT**: Dynamic Token Reduction for Accelerating BERT Inference <a href="https://arxiv.org/abs/2105.11618"><img src="https://img.shields.io/badge/arXiv-2105.11618-b31b1b?logo=arxiv" height="16"></a>  <a href="https://github.com/thunlp/TR-BERT"><img src="https://img.shields.io/github/stars/thunlp/TR-BERT?label=TR_BERT&logo=github&style=flat-square" height="16"></a>

## 2020
- [ICML 2021] Training **data-efficient image transformers & distillation** through attention <a href="https://arxiv.org/abs/2012.12877"><img src="https://img.shields.io/badge/arXiv-2012.12877-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/facebookresearch/deit"><img src="https://img.shields.io/github/stars/facebookresearch/deit?label=deit&logo=github&style=flat-square" height="16"></a>
- [HPCA 2021] **SpAtten**: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning <a href="https://arxiv.org/abs/2012.09852"><img src="https://img.shields.io/badge/arXiv-2012.09852-b31b1b?logo=arxiv" height="16"></a>
- [ICML 2020] **PoWER-BERT**: Accelerating BERT Inference via Progressive Word-vector Elimination <a href="https://arxiv.org/abs/2001.08950"><img src="https://img.shields.io/badge/arXiv-2001.08950-b31b1b?logo=arxiv" height="16"></a> <a href="https://github.com/IBM/PoWER-BERT"><img src="https://img.shields.io/github/stars/IBM/PoWER-BERT?label=PoWER_BERT&logo=github&style=flat-square" height="16"></a>

## ðŸ¤ Contributing
Contributions are welcome! If you find relevant papers or have suggestions, feel free to:
- Submit a [pull request](https://github.com/sangminwoo/awesome-token-reduction/pulls)
- Open an [issue](https://github.com/sangminwoo/awesome-token-reduction/issues)
- Contact me at shmwoo9395@gmail.com

## ðŸ“œ License
[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

This work is licensed under the CC0 1.0 Universal License.
To the extent possible under law, [Sangmin Woo](https://github.com/sangminwoo) has waived all copyright and related or neighboring rights to this work.
